# 秒杀

## 🚀 工业级秒杀架构：Redis + 多线程 Relay + RabbitMQ + DB

1. **极速写入 (Redis)**：依然是抗压前哨。
   
2. **并行搬运 (Relay)**：利用 RabbitMQ 的 Confirm 模式，结合多线程 Relay，实现高吞吐的搬运。
   
3. **可靠缓冲 (RabbitMQ)**：利用 RabbitMQ 的 `durable` (持久化) 队列和 `delivery_mode=2` (消息持久化) 充当蓄水池。
   
    - *注意：RabbitMQ 单机吞吐量低于 Kafka，但在秒杀的“削峰”场景下，只要 Relay 控制好发送速率，完全能扛住百万级订单。*
      
4. **安全消费 (Consumer)**：利用 RabbitMQ 的 `manual ack` (手动确认) 和 `QoS` (预取数量) 保证数据库不被压垮。
   
- 2. 架构组件全览（RabbitMQ 版）
  
    - **Redis (Master)**
      
        - **流量大闸**。库存的唯一事实来源。
          
    - **Redis List (Outbox)**
      
        - **一级缓冲区**。存放 Lua 脚本产生的原始订单消息。
          
    - **Relay Service (中继)**
      
        - **搬运工 (多线程)**。一组带有线程池的无状态进程。负责将 Redis 消息并发、可靠地搬运到 RabbitMQ。
          
    - **Redis (Thread Q)**
      
        - **Relay 线程私有队列**。例如 `processing:queue:thread-01`。防止线程崩溃导致消息丢失。
          
    - **RabbitMQ**
      
        - **二级缓冲区 (持久化)**。开启持久化模式的 Exchange 和 Queue。
          
    - **Order Consumer**
      
        - **下游消费者**。监听 RabbitMQ 队列，手动 Ack，控制消费速度。
          
    - **数据库 (DB)**
      
        - **最终账本**。依靠主键冲突解决重复消息。
          
- 3. 全链路极致详细流程
  
    - 🟢 阶段 0：秒杀预热（不变）
      
        - **动作**：查询 DB 库存 -> 写入 Redis (`SET stock:123 100`)。
          
        - **红线**：Redis 无数据，秒杀不开启。
          
    - 🔴 阶段 1：原子扣减与一级缓冲（Client -> Redis）
      
        - **动作**：前端请求 -> 业务服务器 -> **Redis Lua 脚本**。
          
        - **Lua 逻辑**：
          
            1. `SET NX EX` 检查 `request_id` 幂等性。
               
            2. `DECR` 扣减库存。
               
            3. `LPUSH order:outbox` 写入消息。
               
        - **持久化确认（可选）**：业务服务器调用 `WAIT 1 1000` 防止脑裂丢数据。
          
        - **结果**：用户收到“排队中”。
          
    - 🔵 阶段 2：多线程可靠中继（Redis -> Relay -> RabbitMQ）**[重点]**
      
        - 这是结合了**线程池**和 **RabbitMQ Publisher Confirms** 的设计。
          
        - **配置准备**：
          
            - RabbitMQ 开启 **Publisher Confirms** 模式（确保消息发到 Broker 没丢）。
              
            - Relay 服务内部维护一个 **ThreadPoolExecutor**（比如 20 个线程）。
              
        - **流程详细设计（单个线程内部循环）**：
          
            1. **Step A (私有化拉取)**：
               
                - 线程 T1 执行 `BRPOPLPUSH order:outbox relay:queue:thread-01 2`。
                  
                - *原子性*：消息安全地进入了线程 T1 的私有队列。
                  
            2. **Step B (发送 RabbitMQ)**：
               
                - 线程 T1 调用 `channel.basicPublish(..., props, message)`。
                  
                - **关键配置**：设置 `props.deliveryMode = 2` (持久化消息)。
                  
            3. **Step C (等待 Confirm)**：
               
                - 线程 T1 调用 `channel.waitForConfirms()`。
                  
                - **阻塞等待** RabbitMQ Broker 返回 ACK。只有 Broker 将消息写入磁盘后，才会返回 ACK。
                  
            4. **Step D (清理 Redis)**：
               
                - 收到 RabbitMQ 的 ACK 后，执行 `LREM relay:queue:thread-01 1 "{message}"`。
                  
        - **故障恢复（Crash Recovery）**：
          
            - **故障**：线程 T1 在 Step C 等待时，Relay 服务断电了。
              
            - **后果**：消息没在 RabbitMQ 确认（或者确认了没收到），但在 Redis 的 `relay:queue:thread-01` 里还在。
              
            - **恢复**：
              
                1. Relay 重启。
                   
                2. 初始化线程池。
                   
                3. 线程 T1 启动时，**先不拉新消息**，而是检查 `relay:queue:thread-01`。
                   
                4. 发现遗留消息 -> 重新发送 RabbitMQ -> 等待 Confirm -> 删除 Redis。
                   
                5. *结果*：可能导致 RabbitMQ 里有重复消息（没关系，下游去重）。
                   
    - 🟠 阶段 3：持久化缓冲（RabbitMQ 内部）
      
        - **Exchange 配置**：`durable=true`。
          
        - **Queue 配置**：`durable=true`。
          
        - **Message 配置**：`deliveryMode=2` (Persistent)。
          
        - **作用**：即使 RabbitMQ 宕机重启，消息也不会丢。
          
    - 🟣 阶段 4：平滑消费与最终落地（RabbitMQ -> Consumer -> DB）
      
        - **Consumer 配置**：
          
            - `autoAck = false` (必须关闭自动确认，改为手动确认)。
              
            - `prefetchCount = 50` (QoS 配置，告诉 RabbitMQ：我这里没处理完 50 个之前，别给我发新的)。这能防止数据库被压垮。
              
        - **流程详细设计**：
          
            1. Consumer 拉取一条消息。
               
            2. **Step X (本地事务)**：
               
                - `BEGIN TRANSACTION`
                  
                - `INSERT INTO orders ...` (利用 `request_id` 主键做幂等)。
                  
                - `UPDATE products SET stock ...`
                  
                - `COMMIT`
                  
            3. **Step Y (手动 Ack)**：
               
                - 执行 `channel.basicAck(deliveryTag, false)`。
                  
                - 告诉 RabbitMQ：“这消息我落库了，你可以删了”。
                  
        - **幂等性闭环**：
          
            - 如果 Relay 因为崩溃导致发了两条一样的消息给 RabbitMQ。
              
            - Consumer 处理第二条时，DB 报 **`Duplicate Key`**。
              
            - Consumer 捕获异常，**直接执行 ****`basicAck`**。
              
- 4. 故障处理速查表 (RabbitMQ 版)
  
    - **Redis 挂了**
      
        - 秒杀直接停止
          
        - 哨兵切换，AOF 保证数据不丢，Client 重试。
          
    - **Relay 进程挂了**
      
        - Redis Outbox 堆积
          
        - Relay 重启后，线程扫描各自的 Redis 私有队列，**自动补发** RabbitMQ。
          
    - **RabbitMQ 挂了**
      
        - Relay 发送阻塞
          
        - Relay 线程会在 `waitForConfirms` 处抛异常或阻塞。Relay 会无限重试（或休眠）。Redis Outbox 开始堆积（充当临时缓冲）。
          
    - **Consumer 挂了**
      
        - RabbitMQ 堆积
          
        - RabbitMQ 会保留 Unacked 的消息。Consumer 重启后，RabbitMQ 会**重新投递**这些消息。
          
    - **数据库 挂了**
      
        - Consumer 报错
          
        - Consumer 无法 Ack。消息退回 RabbitMQ 队列头部（或者进入死信交换机 DLX），等待后续处理。
          
- 5. RabbitMQ 特有的注意事项
  
    - 如果您决定用 RabbitMQ，有两点需要特别注意：
      
    1. **Publisher Confirms 的性能**：
       
        - `waitForConfirms()` 是同步阻塞的，性能会比 Kafka 慢。
          
        - **优化**：一定要用**多线程 Relay**。单个线程慢没关系，我们用 20 个、50 个线程并行发，吞吐量完全能撑起秒杀的流量（比如几万 TPS）。
          
    2. **Prefetch Count (QoS)**：
       
        - 在 Consumer 端，一定要设置 `channel.basicQos(N)`。
          
        - 如果设得太大，消费者内存会爆；如果设为 0（无限制），数据库会被打挂。建议设为单次 DB 批量处理能力的 2-3 倍。
          
    - **总结：**  
      用 RabbitMQ 完全没问题！配合 **Redis 多线程 Relay** 模式，您依然拥有一个**高可用、数据零丢失、削峰能力强**的工业级架构。
      
- 流程
  
    - (empty)
      
## 基于上述框架的一人一单的秒杀

### 核心是基于上面的大框架，加入了全局唯一id和幂等性检查的控制

### 全链路极致详细流程

- 🟢 阶段 0：秒杀预热（不变）
  
    - **动作**：查询 DB 库存 -> 写入 Redis (`SET stock:123 100`)。
      
    - **红线**：Redis 无数据，秒杀不开启。
      
- 🔴 阶段 1：原子扣减与一级缓冲（Client -> Redis）
  
    - **动作**：前端请求 -> 业务服务器 -> **Redis Lua 脚本**。
      
        - **位置： 业务服务器（Controller 层）或者网关。 关键点： ID 必须在最开始就生成，绝不能等到进 Redis 或进 MQ 后来生成。**
          
        - 小明点击“立即购买”。
          
        - 服务器接收请求，立马用雪花算法生成一个 ID：ID_8888。
          
        - 现在的请求数据包是：{ "id": "ID_8888", "user": "User_A", "sku": "iPhone" }。
          
        - 为什么不能在消费者端生成？
          
            - 如果小明网络卡了，客户端自动重试了一次。如果在后面生成，系统会以为是两个新订单。如果在入口生成，两次请求带的都是 ID_8888，系统一看就知道是同一个。
              
    - **Lua 逻辑**：
      
        1. **查重（拦截）： Lua 脚本先检查 seckill:users:iPhone 集合里有没有 User_A。**
           
            - 如果没有，说明小明没买过，放行。
              
            - 同时把 User_A 记入集合。
              
        2. 扣库存： Redis 库存 -1。
           
        3. 贴条入列（关键）： 脚本把上面的数据包打包，带着 ID_8888 塞进 Redis 的 outbox 队列 。
           
            - 队列里的数据： "{ "id": "ID_8888", "user": "User_A", ..
              
    - **持久化确认（可选）**：业务服务器调用 `WAIT 1 1000` 防止脑裂丢数据。
      
    - **结果**：用户收到“排队中”。
      
- 🔵 阶段 2：多线程可靠中继（Redis -> Relay -> RabbitMQ）**[重点]**
  
    - 这是结合了**线程池**和 **RabbitMQ Publisher Confirms** 的设计。
      
    - **配置准备**：
      
        - RabbitMQ 开启 **Publisher Confirms** 模式（确保消息发到 Broker 没丢）。
          
        - Relay 服务内部维护一个 **ThreadPoolExecutor**（比如 20 个线程）。
          
    - **流程详细设计（单个线程内部循环）**：
      
        1. **Step A (私有化拉取)**：
           
            - 线程 T1 执行 `BRPOPLPUSH order:outbox relay:queue:thread-01 2`。
              
            - *原子性*：消息安全地进入了线程 T1 的私有队列。
              
        2. **Step B (发送 RabbitMQ)**：
           
            - 线程 T1 调用 `channel.basicPublish(..., props, message)`。
              
            - **关键配置**：设置 `props.deliveryMode = 2` (持久化消息)。
              
        3. **Step C (等待 Confirm)**：
           
            - 线程 T1 调用 `channel.waitForConfirms()`。
              
            - **阻塞等待** RabbitMQ Broker 返回 ACK。只有 Broker 将消息写入磁盘后，才会返回 ACK。
              
        4. **Step D (清理 Redis)**：
           
            - 收到 RabbitMQ 的 ACK 后，执行 `LREM relay:queue:thread-01 1 "{message}"`。
              
        - 这个地方是最容易产生重复消息的，当然，也是不可避免的
          
            - 搬运： Relay 线程从 Redis 拿出 ID_8888 的包裹。
              
            - 备份： 也就是架构图中的 Step 1，Relay 为了安全，先把包裹记在一个小本本上（Redis 里的 backup 队列）。
              
            - 发货： Relay 把包裹投递到 RabbitMQ 。
              
            - 💥 意外发生：
              
                - RabbitMQ 收到了包裹，保存好了。
                  
                - 但是！ Relay 还没来得及划掉小本本上的备份（Step 4），Relay 服务突然断电崩溃了。
                  
            - 结果：
              
                - RabbitMQ 里有一条 ID_8888。
                  
                - Redis 的备份队列里还留着一条 ID_8888 。
                  
            - 重启与重发：
              
                - Relay 服务重启。
                  
                - 它检查小本本（Redis Backup），发现：“咦？这里有个 ID_8888 好像没发成功？”
                  
                - Relay 再次把 ID_8888 发送给 RabbitMQ 。
                  
                - 现状： RabbitMQ 的队列里现在有两条一模一样的消息，ID 都是 ID_8888。
                  
    - **故障恢复（Crash Recovery）**：
      
        - **故障**：线程 T1 在 Step C 等待时，Relay 服务断电了。
          
        - **后果**：消息没在 RabbitMQ 确认（或者确认了没收到），但在 Redis 的 `relay:queue:thread-01` 里还在。
          
        - **恢复**：
          
            1. Relay 重启。
               
            2. 初始化线程池。
               
            3. 线程 T1 启动时，**先不拉新消息**，而是检查 `relay:queue:thread-01`。
               
            4. 发现遗留消息 -> 重新发送 RabbitMQ -> 等待 Confirm -> 删除 Redis。
               
            5. *结果*：可能导致 RabbitMQ 里有重复消息（没关系，下游去重）。
               
- 🟠 阶段 3：持久化缓冲（RabbitMQ 内部）
  
    - **Exchange 配置**：`durable=true`。
      
    - **Queue 配置**：`durable=true`。
      
    - **Message 配置**：`deliveryMode=2` (Persistent)。
      
    - **作用**：即使 RabbitMQ 宕机重启，消息也不会丢。
      
- **🟣 阶段 4：批量入库与最终幂等（Consumer -> DB）**
  
    - 1. 修正数据库表结构：加上“业务唯一索引”
      
        - 仅仅把 `request_id` 设为主键是不够的，那只能防**网络重试**。要防**单人多单**，必须给用户加锁。
          
        - ```sql  
          CREATE TABLE `orders` (  
            `order_id` VARCHAR(64) NOT NULL, -- Request ID (Snowflake)  
            `user_id` BIGINT NOT NULL,  
            `sku_id` INT NOT NULL,  
            `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP,  
              
            PRIMARY KEY (`order_id`), -- 防消息重投（Request级幂等）  
              
            -- 【关键修正】新增业务唯一索引  
            -- 限制：同一个用户，针对同一个商品，只能有一条记录  
            UNIQUE KEY `uk_user_sku` (`user_id`, `sku_id`)   
          ) ENGINE=InnoDB;  
          ```
          
    - 2. 修正 Consumer 逻辑：真正的“双保险”流程
      
        - 当 Consumer 收到消息 `{req: "REQ_002", user: "A", sku: 1001}` 时，它必须面对两个潜在的谎言：
          
        1. **谎言一**：Redis 说该用户没买过（其实 Redis 丢数据了，用户已经买了）。
           
        2. **谎言二**：Redis 说还有库存（其实 Redis 脑裂了，库存其实空了）。
           
        - 消费者必须用以下 SQL 逻辑逐个击破：
          
        - Step X (开启事务)
          
            - `BEGIN TRANSACTION;`
              
        - Step Y (第一关：防单人多单)
          
            - 尝试插入订单，利用数据库的唯一索引做**最终裁决**。
              
            - ```sql  
              INSERT INTO orders (order_id, user_id, sku_id)   
              VALUES ('REQ_002', 'User_A', 1001);  
              ```
              
            - **正常情况**：执行成功。
              
            - **异常情况 1 (消息重投)**：报错 `Duplicate entry 'REQ_002' for key 'PRIMARY'`。
              
                - **处理**：说明是 MQ 重复发了同一个请求。回滚，ACK。
                  
            - **异常情况 2 (Redis 失效导致的用户重买)**：报错 `Duplicate entry 'User_A-1001' for key 'uk_user_sku'`。
              
                - **处理**：说明用户已经买过一个了（哪怕 ID 不一样）。**回滚，ACK**。
                  
                - **结论**：即使 Redis 放行了 10 个 User_A 的请求，只有第一个能入库，剩下 9 个全部撞死在 `uk_user_sku` 上。
                  
        - Step Z (第二关：防超卖)
          
            - 执行库存扣减，利用 `Affected Rows` 做**物理兜底**。
              
            - ```sql  
              -- 【关键修正】必须加上 stock > 0 的条件  
              UPDATE products   
              SET stock = stock - 1   
              WHERE id = 1001 AND stock > 0;   
              ```
              
            - **正常情况**：返回 **Affected Rows = 1**。
              
                - **处理**：说明抢到了真实库存。**COMMIT 事务**。
                  
            - **异常情况 (Redis 脑裂超卖)**：返回 **Affected Rows = 0**。
              
                - **处理**：说明虽然前面的步骤都过了，但数据库里真没货了（Redis 统计错了）。
                  
                - **动作**：**回滚事务**（把刚才 Insert 的订单撤销掉），发送“抢购失败”通知。
                  
    - 3. 完整的“一人一单”兜底流程图
      
        - **1. 开启事务**
          
            - `BEGIN`
              
            - 原子性保证
              
            - -
              
        - **2. 插入订单**
          
            - `INSERT INTO orders ...`
              
            - **防重**：利用 `UNIQUE(user,sku)` 拦截多单。<br>**幂等**：利用 `PK(req_id)` 拦截重试。
              
            - 捕获 `Duplicate Key` 异常 -> **回滚并 ACK**。
              
        - **3. 扣减库存**
          
            - `UPDATE ... WHERE stock > 0`
              
            - **防超卖**：利用数据库行锁和条件做最终一致性检查。
              
            - 检查 `Affected Rows`。<br>如果是 0 -> **回滚并 ACK** (视为库存不足)。
              
        - **4. 提交**
          
            - `COMMIT`
              
            - 只有两关都过，才算成交。
              
            - -
              
        - **5. 确认**
          
            - `ACK`
              
            - 告诉 MQ 任务结束。
              
            - -
              
    - 总结
      
        - 您之前的疑虑是完全正确的。**工业级的安全感不能建立在 Redis 上**。
          
        - **一人一单的保证**：
          
            - **Redis (Cache)**：负责挡住 99.9% 的正常重复流量，保护 DB 不被刷爆。
              
            - **MySQL (Unique Index)**：负责在 Redis 宕机/失效时，物理拦截那 0.1% 的逃逸流量。
              
        - **不超卖的保证**：
          
            - **Redis (DECR)**：负责高性能计数。
              
            - **MySQL (Where stock > 0)**：负责作为“法律依据”，如果没有这一句，Redis 一旦脑裂，数据库就会被扣成负数。
              
        - 加上这两条数据库层面的限制后，这个架构才是真正的**“金刚不坏”**。
          
## 允许一人多单

### 1. 核心设计变更：ID 与 幂等键

- 在不限购场景下，我们不能再用 `SISMEMBER seckill:users` 来拦截用户，因为用户 User_A 买完 5 个，下一秒想再买 3 个是合法的。
  
- **全局唯一 ID (Request ID)**：
  
    - **生成位置**：前端点击“结算”瞬间，或者网关层。
      
    - **算法**：Snowflake（雪花算法）。
      
    - **作用**：代表“这**一次**购买行为”的唯一身份。如果你买了 5 个，这 5 个商品共享这同一个 `Request ID`。
      
- **幂等性策略**：
  
    - **Redis 层**：不再检查“人”，改为检查“这个 ID 是否处理过”。
      
    - **DB 层**：`Request ID` 依然是主键。
      
### 2. 全链路详细流程（支持自定义数量）

- 🟢 阶段 0：秒杀预热
  
    - **动作**：`SET seckill:stock:1001 10000` (存入总库存 10000)。
      
    - **注意**：这里不需要初始化 `seckill:users` 集合了，因为不限购。
      
- 🔴 阶段 1：原子扣减与 Request 级幂等（Client -> Redis）
  
    - 用户 User_A 请求：买 **5** 个，Request ID 为 **`REQ_999`**。
      
    - **Lua 脚本逻辑更新**：  
      以前是检查人，现在必须检查 ID 并在 Redis 留下痕迹（防止网络重试导致多扣）。
      
        - ```lua  
          -- KEYS[1]: 库存Key (seckill:stock:1001)  
          -- KEYS[2]: 幂等记录Key (seckill:req:REQ_999) - 新增！  
          -- KEYS[3]: 消息队列Key (order:outbox)  
            
          -- ARGV[1]: 购买数量 (5)  
          -- ARGV[2]: 消息体 (JSON)  
            
          local stock_key = KEYS[1]  
          local idempotency_key = KEYS[2]  
          local outbox_key = KEYS[3]  
          local quantity = tonumber(ARGV[1])  
          local msg = ARGV[2]  
            
          -- 1. 严格幂等检查：这个 REQ_999 处理过没有？  
          -- 使用 SETNX (Set if Not Exists)  
          -- 如果返回 0，说明这个 ID 已经存在了，直接视为重复请求拦截  
          if redis.call('SETNX', idempotency_key, 1) == 0 then  
              return -1 -- 重复请求  
          end  
          -- 给幂等 Key 设置过期时间（比如 10 分钟），节省内存  
          redis.call('EXPIRE', idempotency_key, 600)  
            
          -- 2. 批量库存检查  
          local current_stock = tonumber(redis.call('GET', stock_key))  
          if current_stock < quantity then  
              -- 重点：如果库存不足，必须把刚才设置的幂等Key删掉？  
              -- 不一定。如果业务认为库存不足也算“处理了一次”，可以不删。  
              -- 但通常为了允许用户改数量重试，建议这里删掉 idempotency_key 或者不删但返回特定错误。  
              -- 简单做法：这里不删，库存不足就是不足，重试也没用。  
              return 0   
          end  
            
          -- 3. 原子扣减 (DECRBY)  
          redis.call('DECRBY', stock_key, quantity)  
            
          -- 4. 写入 Outbox  
          redis.call('LPUSH', outbox_key, msg)  
            
          return 1 -- 成功  
          ```
          
    - **结果**：Redis 内存中多了一个 `seckill:req:REQ_999` 的 Key，库存减少 5。
      
- 🔵 阶段 2：多线程可靠搬运（Redis -> Relay -> RabbitMQ）
  
    - 这一步的逻辑与之前基本一致，**Relay 不需要知道你买几个**，它只负责搬运 JSON 。
      
    - **Step A (私有拉取)**：Relay 线程 `T1` 从 Redis 拉取消息：`{ "req_id": "REQ_999", "uid": "User_A", "count": 5 }`。
      
    - **Step B (发送)**：
      
        - `channel.basicPublish`。
          
        - **关键点**：必须把 `REQ_999` 放入消息属性或作为去重依据。
          
    - **Step C (Confirm)**：等待 RabbitMQ 落盘确认。
      
    - **Step D (清理)**：删除 Redis 里的备份消息 。
      
    - **异常推演**：如果 Relay 发完 `REQ_999` 后挂了，重启后又发了一遍。RabbitMQ 里会有两条 `REQ_999` 的消息。
      
- 🟠 阶段 3：RabbitMQ 缓冲
  
    - **状态**：队列里躺着消息 `{ "req_id": "REQ_999", "count": 5 }`。如果 Relay 重试过，可能躺着两条。
      
- **🟣 阶段 4：批量入库与最终幂等（Consumer -> DB）**
  
    - 1. 表结构调整：拆除人墙，保留防重
      
        - 我们需要调整 `orders` 表的约束。
          
        - **保留** `PRIMARY KEY (order_id)`：这是为了防止**网络重试**和**MQ 消息重投**。每一个具体的“购买动作”依然是唯一的。
          
        - **删除** `UNIQUE KEY (user_id, sku_id)`：因为现在允许 User_A 对 SKU_1001 下单多次。
          
        - ```sql  
          CREATE TABLE `orders` (  
            `order_id` VARCHAR(64) NOT NULL, -- Request ID (Snowflake)  
            `user_id` BIGINT NOT NULL,  
            `sku_id` INT NOT NULL,  
            `count` INT NOT NULL, -- 本次买几个  
            `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP,  
              
            PRIMARY KEY (`order_id`) -- 【兜底一】Request 级幂等，拦截 MQ 重投  
            -- 注意：这里没有针对 user_id 的唯一索引了  
          ) ENGINE=InnoDB;  
          ```
          
    - 2. Consumer 消费逻辑：乐观锁扣减
      
        - 当 Consumer 收到消息 `{req: "REQ_005", user: "A", count: 2}`（User A 想买 2 个）时，数据库的操作流程如下：
          
        - Step 1: 开启事务
          
            - `BEGIN TRANSACTION;`
              
        - Step 2: 幂等性检查 (Request Level)
          
            - 即使允许一人多单，也不能让**同一个请求**执行两次。
              
            - ```sql  
              INSERT INTO orders (order_id, user_id, sku_id, count)   
              VALUES ('REQ_005', 'User_A', 1001, 2);  
              ```
              
            - **兜底逻辑**：
              
                - 如果报错 `Duplicate Key (PRIMARY)`：说明是 **MQ 重复投递**。
                  
                - **动作**：回滚事务，直接 ACK。
                  
        - Step 3: 核心库存兜底 (Stock Level)
          
            - 这是防止 Redis 脑裂导致超卖的**唯一防线**。利用 SQL 的原子性进行扣减。
              
            - ```sql  
              -- 【兜底二】防止超卖的核心 SQL  
              UPDATE products   
              SET stock = stock - 2  -- 扣减本次购买的数量  
              WHERE id = 1001   
                AND stock >= 2;      -- 【关键】必须确保 剩余库存 >= 本次购买量  
              ```
              
            - **正常情况**：库存充足（例如库存是 10，10 >= 2）。
              
                - 返回 **Affected Rows = 1**。
                  
                - **动作**：`COMMIT` 事务，ACK。
                  
            - **异常情况 (Redis 脑裂/数据不一致)**：
              
                - 场景：Redis 以为还有 5 个，放了请求进来。但数据库其实只剩 1 个了。
                  
                - 执行：`UPDATE ... WHERE stock >= 2`。因为 `1 >= 2` 不成立。
                  
                - 结果：返回 **Affected Rows = 0**。
                  
                - **兜底动作**：
                  
                    1. **回滚事务** (`ROLLBACK`)：撤销刚才 Step 2 插入的订单记录（因为没货了，这单不能成）。
                       
                    2. **ACK**：确认消息消费（虽然业务失败了，但处理过程结束了）。
                       
                    3. **告警/通知**：记录日志“Redis 库存与 DB 不一致，触发兜底拦截”。
                       
    - 3. 完整防御逻辑总结表
      
        - **消息重投 / 网络重试**
          
            - Lua `SETNX req_id`
              
            - **有效**
              
            - `PRIMARY KEY (order_id)` 抛出主键冲突。
              
        - **单人多单 (业务允许)**
          
            - **放行**
              
            - **放行**
              
            - 移除了针对 user_id 的唯一索引。
              
        - **超卖 (Redis 脑裂)**
          
            - Lua `DECR`
              
            - **有效 (拦截)**
              
            - `UPDATE ... WHERE stock >= count` 返回 0 行。
              
    - 4. 为什么这样是安全的？
      
        - 哪怕 Redis 挂了，或者 Redis 集群脑裂导致它放进来了 10000 个购买请求（实际库存只有 10 个）。
          
        1. 这 10000 个请求都会进入 MQ。
           
        2. Consumer 会尝试执行 10000 次数据库事务。
           
        3. 前 5 次事务（假设每人买2个），DB 库存足够，`UPDATE` 成功，入库成功。
           
        4. **第 6 次到第 10000 次事务**：
           
            - `INSERT orders` 会成功（因为 ID 不同）。
              
            - 但在执行 `UPDATE products` 时，因为 `stock` 已经是 0 了，`WHERE stock >= 2` 条件永远无法满足。
              
            - 数据库返回 `0` 行受影响。
              
            - 代码逻辑触发 `ROLLBACK`。
              
            - 订单记录被回滚消失。
              
        - **结论**：只要数据库的 `UPDATE ... WHERE stock >= count` 这行代码在，无论 Redis 发生什么灾难级故障，**绝对不会超卖**。
          
### 3. 极端场景问答 (Q&A)

- **Q1: 如果 User_A 先买 5 个，过了一会又买 3 个，会被拦截吗？**
  
    - **Client 端**：生成了两个 ID，分别是 `REQ_999` (5个) 和 `REQ_1000` (3个)。
      
    - **Redis Lua**：
      
        - 检查 `seckill:req:REQ_999` -> 不存在 -> 通过 -> 扣5。
          
        - 检查 `seckill:req:REQ_1000` -> 不存在 -> 通过 -> 扣3。
          
    - **DB 端**：插入两条数据，主键不同，互不冲突。
      
    - **结果**：User_A 成功买了两次，共 8 个。**符合预期**。
      
- **Q2: 如果 User_A 点了“买 5 个”，网络卡了，客户端自动重试了怎么防？**
  
    - **Client 端**：虽然发了两次请求，但因为是“重试”，所以携带的 ID **依然是 ****`REQ_999`**。
      
    - **Redis Lua**：
      
        - 第一次请求到达：`SETNX seckill:req:REQ_999` 成功，扣库存。
          
        - 第二次请求到达：`SETNX seckill:req:REQ_999` **失败**（返回 0）。
          
    - **结果**：Lua 脚本直接返回 -1，第二次请求被丢弃。库存只扣了 5 个。**数据安全**。
      
- **Q3: 为什么 Redis 要设置过期时间 ****`EXPIRE`？**
  
    - 如果不删，Redis 里的 `seckill:req:xxx` 键会越来越多，内存会爆。
      
    - 设置 10 分钟或 1 小时即可。因为秒杀高峰过后，用户不太可能再拿几小时前的旧 ID 来重试。即便来了，数据库的主键也能做最后一道防守。
      
### 总结

- 在**“一人多单 + 自定义数量”**场景下：
  
1. **ID 设计**：ID 代表“请求”而非“用户”。
   
2. **Redis 职责**：利用 `SETNX request_id` 防止**瞬时网络重试**造成的重复扣减；利用 `DECRBY` 处理批量扣减。
   
3. **DB 职责**：利用 `PRIMARY KEY (request_id)` 防止**消息队列重投**造成的重复入库。
   
## 累计限购

### 以前的逻辑是“要么没买，要么买了”（0 或 1），现在的逻辑变成了“你当前已经买了 X 个，还能买 5-X 个”。

### 我们需要引入一个“用户配额计数器 (User Quota Counter)”。整个设计的核心在于：**在 Redis Lua 脚本中同时维护“全局库存”和“个人限购数”的原子性**。

### 以下是针对 **“总限购 5 个，允许分单购买”** 的详细设计方案。

### 1. 核心设计变化

- **全局唯一 ID (Request ID)**：
  
    - 依然是 **Snowflake**。它代表“这一笔订单”（比如买 2 个）。
      
    - **作用**：防止同一笔订单因为网络超时重试，导致用户的配额被扣两次。
      
- **Redis 数据结构新增**：
  
    - `seckill:stock:1001` (String): 全局总库存（例如 10000）。
      
    - `seckill:quota:1001:{user_id}` (String/Hash): **新增！** 记录该用户当前已抢到的总数。
      
    - `seckill:req:{req_id}` (String): 记录处理过的请求 ID（防网络重试）。
      
### 2. 全链路流程详解

- 假设 User_A 已经买过 2 个了，现在发请求想**再买 3 个**（请求 ID: `REQ_002`）。
  
- 🟢 阶段 1：Redis Lua 的“三重原子检查” (Client -> Redis)
  
    - 这是最关键的一步。Lua 脚本必须像通过“三道闸门”一样处理请求。
      
    - **Lua 脚本逻辑（伪代码）：**
      
    - ```lua  
      -- KEYS[1]: 全局库存 Key (seckill:stock:1001)  
      -- KEYS[2]: 用户配额 Key (seckill:quota:1001:User_A)  
      -- KEYS[3]: 请求幂等 Key (seckill:req:REQ_002)  
      -- KEYS[4]: 消息队列 Key (order:outbox)  
        
      -- ARGV[1]: 本次购买数量 (3)  
      -- ARGV[2]: 总限购数量 (5)  
      -- ARGV[3]: 消息体 JSON  
        
      local stock_key = KEYS[1]  
      local quota_key = KEYS[2]  
      local req_key   = KEYS[3]  
      local outbox    = KEYS[4]  
        
      local buy_num   = tonumber(ARGV[1]) -- 3  
      local limit_num = tonumber(ARGV[2]) -- 5  
      local msg       = ARGV[3]  
        
      -- 1. 【第一道闸】请求幂等性检查 (防止网络重试)  
      -- 如果这个 REQ_002 已经处理过，直接返回成功 (幂等)  
      if redis.call('EXISTS', req_key) == 1 then  
          return 1   
      end  
        
      -- 2. 【第二道闸】个人累积限购检查  
      -- 获取用户当前已买数量，如果没有则为 0  
      local current_bought = tonumber(redis.call('GET', quota_key) or "0")  
        
      -- 如果 (已买 + 想买) > 限购数，拒绝  
      if (current_bought + buy_num) > limit_num then  
          return -2 -- 超过限购配额  
      end  
        
      -- 3. 【第三道闸】全局库存检查  
      local global_stock = tonumber(redis.call('GET', stock_key) or "0")  
      if global_stock < buy_num then  
          return 0 -- 库存不足  
      end  
        
      -- 4. 【执行扣减与记录】(原子操作)  
      redis.call('DECRBY', stock_key, buy_num)           -- 扣全局库存  
      redis.call('INCRBY', quota_key, buy_num)           -- 加个人配额 (关键变化)  
      redis.call('SETEX', req_key, 600, 1)               -- 标记请求已处理 (10分钟过期)  
      redis.call('LPUSH', outbox, msg)                   -- 写入 MQ 发货  
        
      return 1 -- 成功  
      ```
      
    - **执行结果推演：**
      
    1. Redis 查到 User_A 之前买过 2 个。
       
    2. 计算 `2 + 3 = 5`，满足 `<= 5` 的条件。
       
    3. Redis 扣减库存，User_A 的配额变为 5。
       
    4. User_A 下次如果想再买 1 个，计算 `5 + 1 = 6`，直接返回 -2 拒绝。
       
- 🔵 阶段 2 & 3：搬运与 MQ (不变)
  
    - Relay 服务和 RabbitMQ 不需要任何改动。它们只负责把“User_A 买 3 个”的消息搬运给消费者。
      
- 🔵 阶段 4. 终极版代码流程 (Consumer -> DB)
  
    - 我们采用 **“幂等先行 + SQL 合并”** 的策略。
      
    - 表结构 (保持不变)
      
        - ```sql  
          CREATE TABLE `user_quota` (  
            `user_id` BIGINT NOT NULL,  
            `sku_id` INT NOT NULL,  
            `owned_count` INT DEFAULT 0,  
            PRIMARY KEY (`user_id`, `sku_id`)  
          ) ENGINE=InnoDB;  
            
          CREATE TABLE `orders` (  
            `order_id` VARCHAR(64) NOT NULL, -- Request ID  
            ...  
            PRIMARY KEY (`order_id`)  
          ) ENGINE=InnoDB;  
          ```
          
    - 消费者执行逻辑 (伪代码 + SQL)
      
        - Consumer 收到消息：`{req: "REQ_01", user: "A", count: 2}`
          
        - **Step 1: 开启事务**  
          `BEGIN;`
          
        - **Step 2: 幂等性“安检” (第一道门)**  
          先尝试插入订单。如果这是个重试请求，在这里就应该停下。
          
        - ```sql  
          INSERT INTO orders (order_id, user_id, sku_id, count)   
          VALUES ('REQ_01', 'User_A', 1001, 2);  
          ```
          
        - **异常处理**：
          
            - 如果报 `Duplicate entry ... PRIMARY`：说明是**重复消费**。
              
            - **动作**：`ROLLBACK` -> **直接 ACK**（视为成功）。不要往下走了。
              
        - **Step 3: 累计限购检查 + 扣减 (第二道门)**  
          这里我们使用 `INSERT ON DUPLICATE KEY UPDATE` 的高级用法，将“初始化”和“更新”合并为一条 SQL，原子性更强。
          
        - ```sql  
          -- 尝试插入或更新配额  
          INSERT INTO user_quota (user_id, sku_id, owned_count)   
          VALUES ('User_A', 1001, 2) -- 2 是本次想买的数量(作为初始值)  
          ON DUPLICATE KEY UPDATE   
              -- 【关键逻辑】如果 (已有 + 本次) <= 5，则更新；否则保持原样  
              owned_count = IF(owned_count + 2 <= 5, owned_count + 2, owned_count);  
          ```
          
        - **检查结果 (Affected Rows)**：
          
            - **1**：新用户，插入成功（0 -> 2）。**通过**。
              
            - **2**：老用户，更新成功（例如 3 -> 5）。**通过**。
              
            - **0**：老用户，更新失败（例如 4 + 2 > 5，值未变）。**拦截！**
              
        - **动作**：如果是 **0**，说明超限。`ROLLBACK` -> **ACK** (业务失败)。
          
        - **Step 4: 全局库存扣减 (第三道门)**
          
        - ```sql  
          UPDATE products   
          SET stock = stock - 2   
          WHERE id = 1001 AND stock >= 2;  
          ```
          
        - **检查结果**：
          
            - **1**：库存充足。**通过**。
              
            - **0**：库存不足（Redis 脑裂了）。**拦截！**
              
        - **动作**：如果是 **0**，说明没货。`ROLLBACK` -> **ACK**。
          
        - **Step 5: 提交事务**  
          `COMMIT;` -> **ACK**。
          
- 场景推演（新逻辑验证）
  
    - 假设：限购 5 个。
      
    - 场景 A：正常购买（已买 3，买 2）
      
        1. `INSERT orders`: 成功。
           
        2. `INSERT quota ... UPDATE`:
           
            - 原值为 3。条件 `3+2 <= 5` 成立。
              
            - 更新为 5。
              
            - **MySQL 返回：2** (Row updated)。-> **通过**。
              
        3. `UPDATE stock`: 返回 1。-> **通过**。
           
        4. `COMMIT`: **成交**。
           
    - 场景 B：超限购买（已买 4，买 2）
      
        1. `INSERT orders`: 成功。
           
        2. `INSERT quota ... UPDATE`:
           
            - 原值为 4。条件 `4+2 <= 5` **不成立**。
              
            - 执行 `owned_count = owned_count` (保持 4)。
              
            - **MySQL 返回：0** (No change)。-> **触发失败**。
              
        3. `ROLLBACK`: 刚才插入的 `orders` 记录被撤销。
           
        4. 结果：**下单失败**。符合预期。
           
    - 场景 C：网络重试（重复请求）
      
        - *背景：场景 A 实际上已经成功了，但 ACK 丢了，MQ 重发消息。*
          
        1. `INSERT orders`:
           
            - **MySQL 报错**：`Duplicate entry 'REQ_01'`。
              
        2. `Catch Exception`: 捕获到主键冲突。
           
        3. `ROLLBACK`: (虽然没做啥，习惯性回滚)。
           
        4. `ACK`: 告诉 MQ “我处理完了”（其实是幂等成功）。
           
        5. 结果：**用户看到“购买成功”**。
           
            - *注：如果是旧逻辑，这里会去检查 Quota，发现已买 5 个，无法再加 2 个，报错“限购已满”，用户体验崩塌。*
              
### 3. 常见“坑”与解决方案

- Q1: 如果 Redis 的用户配额数据丢了怎么办？(Redis 崩溃/重启)
  
    - **场景**：User_A 在 Redis 里记录买了 5 个。Redis 挂了，数据没持久化（或 AOF 没来得及刷盘）。Redis 重启后，User_A 的配额 Key 消失了。  
      **后果**：User_A 可以再买 5 个。总共买了 10 个（超限）。  
      **解决方案**：
      
    - **方案 A (容忍)**：对于秒杀活动，稍微超限几个通常是可以接受的（只要总库存不超卖）。这是最高性能做法。
      
    - **方案 B (预热加载)**：秒杀开始前把 VIP 用户的购买记录预热进 Redis（不现实）。
      
    - **方案 C (DB 校验)**：在 Lua 里不做处理，但在 Consumer 入库时，利用数据库 sum 校验（`SELECT SUM(count) FROM orders WHERE user_id = A`）。如果超限，回滚并**不发货**（不仅要回滚 DB，还要异步补偿 Redis 库存）。—— **这太复杂，不推荐用于高并发秒杀，建议选方案 A。**
      
- Q2: 为什么一定要先检查 Request ID (幂等)，再检查配额？
  
    - **错误顺序演示**：
      
    1. **先加配额**：User_A (已买2个) 发送请求 `REQ_002` (买3个)。Lua 先执行 `INCRBY`，配额变成 5。
       
    2. **网络中断**：客户端没收到响应，以为失败了。
       
    3. **重试**：客户端再次发送 `REQ_002`。
       
    4. **再次加配额**：Lua 执行 `INCRBY`，配额变成 5+3=8。
       
    5. **判断**：8 > 5，Lua 报错“配额已满”。  
       **结果**：User_A 明明只买了 5 个，却因为一次重试，导致系统认为他超限了，这单交易失败了（也就是配额被“吞”了）。  
       **正确顺序**：先检查 `seckill:req:REQ_002` 是否存在。如果存在，说明刚才加过配额了，直接返回成功。
       
### 总结图表

- (empty)
  
    - **1. 幂等**
      
        - `req_id`
          
        - 之前处理过这个 ID 吗？
          
        - (无，直接通过)
          
        - **直接返回成功** (不扣库存，不加配额)
          
    - **2. 配额**
      
        - `user_quota`
          
        - 已买 + 本次 > 5 ?
          
        - **返回 "限购已满"**
          
        - 内存中准备加数
          
    - **3. 库存**
      
        - `global_stock`
          
        - 剩余库存 < 本次 ?
          
        - **返回 "已抢光"**
          
        - 内存中准备扣数
          
    - **4. 执行**
      
        - Redis Write
          
        - **原子执行**：<br>1. 记录 req_id<br>2. 增加 quota<br>3. 扣减 stock<br>4. 推送 Outbox
          
        - -
          
        - 返回 1
          
- 通过这个设计，您可以在支持**“高并发”**的同时，完美支持**“累积限购”**和**“分单购买”**，且数据严格一致。
  
## 关于脑裂的解决方法

### 1. 战术层面：强制同步与“宁杀错不放过” (Strict Config)（太影响性能了，没啥必要）

- 虽然 `WAIT` 不是银弹，但通过极端的配置，我们可以将系统置于一个**“极度保守”**的状态。这符合你说的“拒绝也行”。
  
- **配置策略**：
  
    - **`min-replicas-to-write 1` (或更多)**：在 `redis.conf` 中配置。如果不满足至少 1 个 Slave 在线并同步，Master **直接拒绝写入**。这从源头阻止了“孤岛 Master”继续接收写请求。
      
    - **`WAIT 1 1000` 的错误处理**：业务服务器执行 Lua 脚本后，调用 `WAIT`。
      
        - **关键点**：如果 `WAIT` 超时（返回的已同步副本数 < 要求数），业务代码**必须**认为**交易失败**（即使 Redis Master 上可能已经扣减成功）。
          
        - **回滚/标记**：给用户报错“繁忙”，同时后台可以尝试发送一个“补偿/回滚”消息（虽然在脑裂场景下回滚可能也发不出去，但这是一个态度）。
          
- **效果**：这会导致大量的**“少卖”**（Underselling）。
  
    - 例如：Master 扣了，Slave 其实也收到了，但 ACK 慢了。系统判定失败，用户没买到。
      
    - **大厂态度**：少卖没问题，库存还在那，等流量洪峰过了或者人工补货还能卖。**超卖是绝对的技术事故。**
      
### 2. 架构层面：库存分桶 (Stock Sharding) —— 必杀技（要是有机器可以这么用）

- 这是大厂解决 Redis 集群热点和单点故障（包括脑裂风险）最常用的**物理隔离**手段。
  
- **原理**：  
  不要把 10000 个 iPhone 的库存全放在一个 Key (`stock:1001`) 上，而是拆分成 100 个 Key，分散在 Redis 集群的**不同节点（Shards）** 上。
  
    - `stock:1001:0` -> 100 个
      
    - `stock:1001:1` -> 100 个
      
    - ...
      
    - `stock:1001:99` -> 100 个
      
- **流程**：
  
    1. 用户请求进来，通过 `hash(user_id) % 100` 计算出该用户应该去哪个桶扣库存。
       
    2. 去对应的 Redis 节点执行 Lua 扣减。
       
- **如何防脑裂超卖？**
  
    - **风险稀释**：如果 Redis 集群中某一个 Master 节点发生了“脑裂”或宕机回滚，**它只影响这 1/100 的库存**。
      
    - **损失控制**：假设该节点发生故障，回滚了 5 秒的数据。在单 Key 模式下，你可能超卖 5000 单；在分桶模式下，你只超卖了 50 单。对于大厂的体量，几十单的资损是可以通过“客户关怀”（发优惠券、协商退款）低成本解决的。
      
### 3. 终极防守：数据库兜底 (DB Constraint) —— 真正的守门员（也是我们的架构使用的）

- 既然你提到了“Redis 是基准，异步下单”，那么 Redis 其实只是一个**流量漏斗**。真正的**“账本”**依然是数据库。
  
- 即使 Redis 脑裂了，放进来了 105 个人（实际库存 100），这 105 个消息最终会由 Relay 搬运到 Consumer，最后尝试插入数据库。
  
- **数据库层面的“乐观锁”或“约束”**：  
  在 Consumer 扣减数据库真实库存表时，SQL 必须这么写：
  
    - ```sql  
      UPDATE product_stock   
      SET stock = stock - 1   
      WHERE sku_id = 1001 AND stock > 0; -- 关键：stock > 0  
      ```
      
- **流程推演**：
  
    1. Redis 脑裂，多放了 5 个人进来（Redis 库存变成了负数或者回滚了，反正发出了 105 个 Outbox 消息）。
       
    2. MQ 里有 105 个订单消息。
       
    3. Consumer 处理前 100 个：成功，数据库库存归零。
       
    4. Consumer 处理第 101 个：执行上面的 SQL，`Affected Rows` 为 0（因为 `stock > 0` 不满足）。
       
    5. **处理结果**：Consumer 捕获到“库存不足”的情况，**标记该订单失败**（或者进入“排队超时”状态），并给用户发送“抱歉，抢购失败”的通知（虽然用户之前在前台看到了“排队中”甚至“抢购成功”，但最终结果以 DB 为准）。
       
- **这是大厂的底线逻辑：**  
  前端和 Redis 层的“成功”只是**“获得了入场券”**，而不是**“成交”**。最终成交以数据库扣减成功为准。只要数据库不松口，Redis 再怎么脑裂，也不会产生实际的**资金/货物超卖**，只会产生**“用户体验上的超卖”**（用户以为买到了，最后告诉他没货）。
  
