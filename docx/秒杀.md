# 秒杀

## 🚀 工业级秒杀架构：Redis + 多线程 Relay + RabbitMQ + DB框架（具体分为下面三个细节场景）

1. **极速写入 (Redis)**：依然是抗压前哨。
   
2. **并行搬运 (Relay)**：利用 RabbitMQ 的 Confirm 模式，结合多线程 Relay，实现高吞吐的搬运。
   
3. **可靠缓冲 (RabbitMQ)**：利用 RabbitMQ 的 `durable` (持久化) 队列和 `delivery_mode=2` (消息持久化) 充当蓄水池。
   
    - *注意：RabbitMQ 单机吞吐量低于 Kafka，但在秒杀的“削峰”场景下，只要 Relay 控制好发送速率，完全能扛住百万级订单。*
      
4. **安全消费 (Consumer)**：利用 RabbitMQ 的 `manual ack` (手动确认) 和 `QoS` (预取数量) 保证数据库不被压垮。
   
- 2. 架构组件全览（RabbitMQ 版）
  
    - **Redis (Master)**
      
        - **流量大闸**。库存的唯一事实来源。
          
    - **Redis List (Outbox)**
      
        - **一级缓冲区**。存放 Lua 脚本产生的原始订单消息。
          
    - **Relay Service (中继)**
      
        - **搬运工 (多线程)**。一组带有线程池的无状态进程。负责将 Redis 消息并发、可靠地搬运到 RabbitMQ。
          
    - **Redis (Thread Q)**
      
        - **Relay 线程私有队列**。例如 `processing:queue:thread-01`。防止线程崩溃导致消息丢失。
          
    - **RabbitMQ**
      
        - **二级缓冲区 (持久化)**。开启持久化模式的 Exchange 和 Queue。
          
    - **Order Consumer**
      
        - **下游消费者**。监听 RabbitMQ 队列，手动 Ack，控制消费速度。
          
    - **数据库 (DB)**
      
        - **最终账本**。依靠主键冲突解决重复消息。
          
- 3. 全链路极致详细流程
  
    - 🟢 阶段 0：秒杀预热（不变）
      
        - **动作**：查询 DB 库存 -> 写入 Redis (`SET stock:123 100`)。
          
        - **红线**：Redis 无数据，秒杀不开启。
          
    - 🔴 阶段 1：原子扣减与一级缓冲（Client -> Redis）
      
        - **动作**：前端请求 -> 业务服务器 -> **Redis Lua 脚本**。
          
        - **Lua 逻辑**：
          
            1. `SET NX EX` 检查 `request_id` 幂等性。
               
            2. `DECR` 扣减库存。
               
            3. `LPUSH order:outbox` 写入消息。
               
        - **持久化确认（可选）**：业务服务器调用 `WAIT 1 1000` 防止脑裂丢数据。
          
        - **结果**：用户收到“排队中”。
          
    - 🔵 阶段 2：多线程可靠中继（Redis -> Relay -> RabbitMQ）**[重点]**
      
        - 这是结合了**线程池**和 **RabbitMQ Publisher Confirms** 的设计。
          
        - **配置准备**：
          
            - RabbitMQ 开启 **Publisher Confirms** 模式（确保消息发到 Broker 没丢）。
              
            - Relay 服务内部维护一个 **ThreadPoolExecutor**（比如 20 个线程）。
              
        - **流程详细设计（单个线程内部循环）**：
          
            1. **Step A (私有化拉取)**：
               
                - 线程 T1 执行 `BRPOPLPUSH order:outbox relay:queue:thread-01 2`。
                  
                - *原子性*：消息安全地进入了线程 T1 的私有队列。
                  
            2. **Step B (发送 RabbitMQ)**：
               
                - 线程 T1 调用 `channel.basicPublish(..., props, message)`。
                  
                - **关键配置**：设置 `props.deliveryMode = 2` (持久化消息)。
                  
            3. **Step C (等待 Confirm)**：
               
                - 线程 T1 调用 `channel.waitForConfirms()`。
                  
                - **阻塞等待** RabbitMQ Broker 返回 ACK。只有 Broker 将消息写入磁盘后，才会返回 ACK。
                  
            4. **Step D (清理 Redis)**：
               
                - 收到 RabbitMQ 的 ACK 后，执行 `LREM relay:queue:thread-01 1 "{message}"`。
                  
        - **故障恢复（Crash Recovery）**：
          
            - **故障**：线程 T1 在 Step C 等待时，Relay 服务断电了。
              
            - **后果**：消息没在 RabbitMQ 确认（或者确认了没收到），但在 Redis 的 `relay:queue:thread-01` 里还在。
              
            - **恢复**：
              
                1. Relay 重启。
                   
                2. 初始化线程池。
                   
                3. 线程 T1 启动时，**先不拉新消息**，而是检查 `relay:queue:thread-01`。
                   
                4. 发现遗留消息 -> 重新发送 RabbitMQ -> 等待 Confirm -> 删除 Redis。
                   
                5. *结果*：可能导致 RabbitMQ 里有重复消息（没关系，下游去重）。
                   
    - 🟠 阶段 3：持久化缓冲（RabbitMQ 内部）
      
        - **Exchange 配置**：`durable=true`。
          
        - **Queue 配置**：`durable=true`。
          
        - **Message 配置**：`deliveryMode=2` (Persistent)。
          
        - **作用**：即使 RabbitMQ 宕机重启，消息也不会丢。
          
    - 🟣 阶段 4：平滑消费与最终落地（RabbitMQ -> Consumer -> DB）
      
        - **Consumer 配置**：
          
            - `autoAck = false` (必须关闭自动确认，改为手动确认)。
              
            - `prefetchCount = 50` (QoS 配置，告诉 RabbitMQ：我这里没处理完 50 个之前，别给我发新的)。这能防止数据库被压垮。
              
        - **流程详细设计**：
          
            1. Consumer 拉取一条消息。
               
            2. **Step X (本地事务)**：
               
                - `BEGIN TRANSACTION`
                  
                - `INSERT INTO orders ...` (利用 `request_id` 主键做幂等)。
                  
                - `UPDATE products SET stock ...`
                  
                - `COMMIT`
                  
            3. **Step Y (手动 Ack)**：
               
                - 执行 `channel.basicAck(deliveryTag, false)`。
                  
                - 告诉 RabbitMQ：“这消息我落库了，你可以删了”。
                  
        - **幂等性闭环**：
          
            - 如果 Relay 因为崩溃导致发了两条一样的消息给 RabbitMQ。
              
            - Consumer 处理第二条时，DB 报 **`Duplicate Key`**。
              
            - Consumer 捕获异常，**直接执行 ****`basicAck`**。
              
- 4. 故障处理速查表 (RabbitMQ 版)
  
    - **Redis 挂了**
      
        - 秒杀直接停止
          
        - 哨兵切换，AOF 保证数据不丢，Client 重试。
          
    - **Relay 进程挂了**
      
        - Redis Outbox 堆积
          
        - Relay 重启后，线程扫描各自的 Redis 私有队列，**自动补发** RabbitMQ。
          
    - **RabbitMQ 挂了**
      
        - Relay 发送阻塞
          
        - Relay 线程会在 `waitForConfirms` 处抛异常或阻塞。Relay 会无限重试（或休眠）。Redis Outbox 开始堆积（充当临时缓冲）。
          
    - **Consumer 挂了**
      
        - RabbitMQ 堆积
          
        - RabbitMQ 会保留 Unacked 的消息。Consumer 重启后，RabbitMQ 会**重新投递**这些消息。
          
    - **数据库 挂了**
      
        - Consumer 报错
          
        - Consumer 无法 Ack。消息退回 RabbitMQ 队列头部（或者进入死信交换机 DLX），等待后续处理。
          
- 5. RabbitMQ 特有的注意事项
  
    - 如果您决定用 RabbitMQ，有两点需要特别注意：
      
    1. **Publisher Confirms 的性能**：
       
        - `waitForConfirms()` 是同步阻塞的，性能会比 Kafka 慢。
          
        - **优化**：一定要用**多线程 Relay**。单个线程慢没关系，我们用 20 个、50 个线程并行发，吞吐量完全能撑起秒杀的流量（比如几万 TPS）。
          
    2. **Prefetch Count (QoS)**：
       
        - 在 Consumer 端，一定要设置 `channel.basicQos(N)`。
          
        - 如果设得太大，消费者内存会爆；如果设为 0（无限制），数据库会被打挂。建议设为单次 DB 批量处理能力的 2-3 倍。
          
    - **总结：**  
      用 RabbitMQ 完全没问题！配合 **Redis 多线程 Relay** 模式，您依然拥有一个**高可用、数据零丢失、削峰能力强**的工业级架构。
      
- 流程
  
    - (empty)
      
## 基于上述框架的一人一单的秒杀

### 核心是基于上面的大框架，加入了全局唯一id和幂等性检查的控制

### 全链路极致详细流程

- 🟢 阶段 0：秒杀预热（不变）
  
    - **动作**：查询 DB 库存 -> 写入 Redis (`SET stock:123 100`)。
      
    - **红线**：Redis 无数据，秒杀不开启。
      
- 🔴 阶段 1：原子扣减与一级缓冲（Client -> Redis）
  
    - **动作**：前端请求 -> 业务服务器 -> **Redis Lua 脚本**。
      
        - **位置： 业务服务器（Controller 层）或者网关。 关键点： ID 必须在最开始就生成，绝不能等到进 Redis 或进 MQ 后来生成。**
          
        - 小明点击“立即购买”。
          
        - ~~服务器接收请求，立马用雪花算法生成一个 ID：ID_8888。~~
          
            - 这为什么是个错解？
              
                - 因为我们要回查！这回id在服务器生成了，redis的ack丢了该怎么办，重试吗？老的id怎么办，我是用新的id回查还是老的id回查啊
                  
                    - ❌ 问题 1：ID 生成策略与“一人一单”的逻辑冲突
                      
                        - **你现在的逻辑：**
                          
                        1. 用户点击 -> 服务器生成 **Snowflake ID (ID_A)**。
                           
                        2. Redis 检查 `User_A` 是否在集合里。
                           
                        3. 如果不都在 -> 扣减，加入集合。
                           
                        4. 如果 **响应丢包**（Redis 成功了但没告诉前端），前端超时。
                           
                        5. 用户重试 -> 服务器生成 **Snowflake ID (ID_B)**。
                           
                        6. Redis 检查 `User_A` 在集合里 -> **报错“您已购买”**。
                           
                        - **隐患：**  
                          用户看到的是“请求超时”，点了重试却被告知“已购买”。
                          
                        - **用户视角**：“我明明没买到（前端没显示成功），凭什么说我买过了？”
                          
                        - **后果**：用户不知道订单号是啥（ID_A 丢了），前端也没拿到 ID_A，用户想查单都不知道怎么查，只能去订单列表碰运气。
                          
                        - **效果**：
                          
                            - 第一次请求：生成 `Hash_Key_X`。Redis 执行成功。
                              
                            - 响应丢包，用户重试。
                              
                            - 第二次请求：依然生成 `Hash_Key_X`。
                              
                            - **Redis 逻辑变更**：Lua 脚本不再只是检查“人”，而是检查 `SETNX Key_X`。
                              
                            - **结果**：发现 Key 已存在，**视为“幂等成功”**，直接返回成功！
                              
                            - **用户体验**：第一次超时，第二次点立刻提示“抢购成功”，无缝衔接。
                              
                - **✅ 修正建议：放弃 Snowflake，使用“确定性 ID”**  
                  在“一人一单”场景下，不要用时间戳随机生成 ID，要用算法生成 **“命中注定”** 的 ID。
                  
                    - **生成公式**：`Order_ID = MD5("Activity_1001" + "User_A")`
                      
                    - 你要是连md5的hash碰撞都容忍不了，干脆直接Activity_1001 + User_A
                      
        - 现在的请求数据包是：{ "id": "ID_8888", "user": "User_A", "sku": "iPhone" }。
          
        - 为什么不能在消费者端生成？
          
            - 如果小明网络卡了，客户端自动重试了一次。如果在后面生成，系统会以为是两个新订单。如果在入口生成，两次请求带的都是 ID_8888，系统一看就知道是同一个。
              
    - **Lua 逻辑**：
      
        1. **查重（拦截）： Lua 脚本先检查 seckill:users:iPhone 集合里有没有 User_A。**
           
            - 如果没有，说明小明没买过，放行。
              
            - 同时把 User_A 记入集合。
              
        2. 扣库存： Redis 库存 -1。
           
        3. 贴条入列（关键）： 脚本把上面的数据包打包，带着 ID_8888 塞进 Redis 的 outbox 队列 。
           
            - 队列里的数据： "{ "id": "ID_8888", "user": "User_A", ..
              
    - **持久化确认（可选）**：业务服务器调用 `WAIT 1 1000` 防止脑裂丢数据。
      
    - **结果**：用户收到“排队中”。
      
- 🔵 阶段 2：多线程可靠中继（Redis -> Relay -> RabbitMQ）**[重点]**
  
    - 这是结合了**线程池**和 **RabbitMQ Publisher Confirms** 的设计。
      
    - **配置准备**：
      
        - RabbitMQ 开启 **Publisher Confirms** 模式（确保消息发到 Broker 没丢）。
          
        - Relay 服务内部维护一个 **ThreadPoolExecutor**（比如 20 个线程）。
          
    - **流程详细设计（单个线程内部循环）**：
      
        1. **Step A (私有化拉取)**：
           
            - 线程 T1 执行 `BRPOPLPUSH order:outbox relay:queue:thread-01 2`。
              
            - *原子性*：消息安全地进入了线程 T1 的私有队列。
              
        2. **Step B (发送 RabbitMQ)**：
           
            - 线程 T1 调用 `channel.basicPublish(..., props, message)`。
              
            - **关键配置**：设置 `props.deliveryMode = 2` (持久化消息)。
              
        3. **Step C (等待 Confirm)**：
           
            - 线程 T1 调用 `channel.waitForConfirms()`。
              
            - **阻塞等待** RabbitMQ Broker 返回 ACK。只有 Broker 将消息写入磁盘后，才会返回 ACK。
              
        4. **Step D (清理 Redis)**：
           
            - 收到 RabbitMQ 的 ACK 后，执行 `LREM relay:queue:thread-01 1 "{message}"`。
              
        - 这个地方是最容易产生重复消息的，当然，也是不可避免的
          
            - 搬运： Relay 线程从 Redis 拿出 ID_8888 的包裹。
              
            - 备份： 也就是架构图中的 Step 1，Relay 为了安全，先把包裹记在一个小本本上（Redis 里的 backup 队列）。
              
            - 发货： Relay 把包裹投递到 RabbitMQ 。
              
            - 💥 意外发生：
              
                - RabbitMQ 收到了包裹，保存好了。
                  
                - 但是！ Relay 还没来得及划掉小本本上的备份（Step 4），Relay 服务突然断电崩溃了。
                  
            - 结果：
              
                - RabbitMQ 里有一条 ID_8888。
                  
                - Redis 的备份队列里还留着一条 ID_8888 。
                  
            - 重启与重发：
              
                - Relay 服务重启。
                  
                - 它检查小本本（Redis Backup），发现：“咦？这里有个 ID_8888 好像没发成功？”
                  
                - Relay 再次把 ID_8888 发送给 RabbitMQ 。
                  
                - 现状： RabbitMQ 的队列里现在有两条一模一样的消息，ID 都是 ID_8888。
                  
    - **故障恢复（Crash Recovery）**：
      
        - **故障**：线程 T1 在 Step C 等待时，Relay 服务断电了。
          
        - **后果**：消息没在 RabbitMQ 确认（或者确认了没收到），但在 Redis 的 `relay:queue:thread-01` 里还在。
          
        - **恢复**：
          
            1. Relay 重启。
               
            2. 初始化线程池。
               
            3. 线程 T1 启动时，**先不拉新消息**，而是检查 `relay:queue:thread-01`。
               
            4. 发现遗留消息 -> 重新发送 RabbitMQ -> 等待 Confirm -> 删除 Redis。
               
            5. *结果*：可能导致 RabbitMQ 里有重复消息（没关系，下游去重）。
               
- 🟠 阶段 3：持久化缓冲（RabbitMQ 内部）
  
    - **Exchange 配置**：`durable=true`。
      
    - **Queue 配置**：`durable=true`。
      
    - **Message 配置**：`deliveryMode=2` (Persistent)。
      
    - **作用**：即使 RabbitMQ 宕机重启，消息也不会丢。
      
- **🟣 阶段 4：批量入库与最终幂等（Consumer -> DB）**
  
    - 1. 修正数据库表结构：加上“业务唯一索引”
      
        - 仅仅把 `request_id` 设为主键是不够的，那只能防**网络重试**。要防**单人多单**，必须给用户加锁。
          
        - ```sql  
          CREATE TABLE `orders` (  
            `order_id` VARCHAR(64) NOT NULL, -- Request ID (Snowflake)  
            `user_id` BIGINT NOT NULL,  
            `sku_id` INT NOT NULL,  
            `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP,  
              
            PRIMARY KEY (`order_id`), -- 防消息重投（Request级幂等）  
              
            -- 【关键修正】新增业务唯一索引  
            -- 限制：同一个用户，针对同一个商品，只能有一条记录  
            UNIQUE KEY `uk_user_sku` (`user_id`, `sku_id`)   
          ) ENGINE=InnoDB;  
          ```
          
    - 2. 修正 Consumer 逻辑：真正的“双保险”流程
      
        - 当 Consumer 收到消息 `{req: "REQ_002", user: "A", sku: 1001}` 时，它必须面对两个潜在的谎言：
          
        1. **谎言一**：Redis 说该用户没买过（其实 Redis 丢数据了，用户已经买了）。
           
        2. **谎言二**：Redis 说还有库存（其实 Redis 脑裂了，库存其实空了）。
           
        - 消费者必须用以下 SQL 逻辑逐个击破：
          
        - Step X (开启事务)
          
            - `BEGIN TRANSACTION;`
              
        - Step 1 (第一关：防单人多单)
          
            - 尝试插入订单，利用数据库的唯一索引做**最终裁决**。
              
            - ```sql  
              INSERT INTO orders (order_id, user_id, sku_id)   
              VALUES ('REQ_002', 'User_A', 1001);  
              ```
              
            - **正常情况**：执行成功。
              
            - **异常情况 1 (消息重投)**：报错 `Duplicate entry 'REQ_002' for key 'PRIMARY'`。
              
                - **处理**：说明是 MQ 重复发了同一个请求。回滚，ACK。
                  
            - **异常情况 2 (Redis 失效导致的用户重买)**：报错 `Duplicate entry 'User_A-1001' for key 'uk_user_sku'`。
              
                - **处理**：说明用户已经买过一个了（哪怕 ID 不一样）。**回滚，ACK**。
                  
                - **结论**：即使 Redis 放行了 10 个 User_A 的请求，只有第一个能入库，剩下 9 个全部撞死在 `uk_user_sku` 上。
                  
        - Step 2 (第二关：防超卖)
          
            - 执行库存扣减，利用 `Affected Rows` 做**物理兜底**。
              
            - ```sql  
              -- 【关键修正】必须加上 stock > 0 的条件  
              UPDATE products   
              SET stock = stock - 1   
              WHERE id = 1001 AND stock > 0;   
              ```
              
            - **正常情况**：返回 **Affected Rows = 1**。
              
                - **处理**：说明抢到了真实库存。**COMMIT 事务**。
                  
            - **异常情况 (Redis 脑裂超卖)**：返回 **Affected Rows = 0**。
              
                - **处理**：说明虽然前面的步骤都过了，但数据库里真没货了（Redis 统计错了）。
                  
                - **动作**：**回滚事务**（把刚才 Insert 的订单撤销掉），发送“抢购失败”通知。
                  
        - Step 3 
          
            - **回写redis：**然后回写redis某个键为finish，库存减没减这单都成了，要是设置redis失败了可以象征性重试几次，最差也就被认为not found呗，记得设置超时时间为被认为Not found的限制时间
              
            - 隐患（千万别回表）： 这是绝对禁止的。 在秒杀结束后的几秒内，可能有 10 万人同时抢到了商品，同时轮询出了 SUCCESS 状态。 如果这 10 万人同时发起 SELECT * FROM orders WHERE id = ...，数据库必挂无疑。哪怕是查主键，瞬间 10w QPS 的并发也是 MySQL 扛不住的。
              
                - ✅ 修正建议：Redis 结果即真理 Consumer 回写 Redis 的数据结构必须包含所有必要信息，让前端不需要查 DB 就能展示“抢购成功页”。
                  
            - 就算真not found了，也让用户去“我的订单”里找，在那里查db
              
    - 3. 完整的“一人一单”兜底流程图
      
        - **1. 开启事务**
          
            - `BEGIN`
              
            - 原子性保证
              
            - -
              
        - **2. 插入订单**
          
            - `INSERT INTO orders ...`
              
            - **防重**：利用 `UNIQUE(user,sku)` 拦截多单。<br>**幂等**：利用 `PK(req_id)` 拦截重试。
              
            - 捕获 `Duplicate Key` 异常 -> **回滚并 ACK**。
              
        - **3. 扣减库存**
          
            - `UPDATE ... WHERE stock > 0`
              
            - **防超卖**：利用数据库行锁和条件做最终一致性检查。
              
            - 检查 `Affected Rows`。<br>如果是 0 -> **回滚并 ACK** (视为库存不足)。
              
        - **4. 提交**
          
            - `COMMIT`
              
            - 只有两关都过，才算成交。
              
            - -
              
        - **5. 确认**
          
            - `ACK`
              
            - 告诉 MQ 任务结束。
              
            - -
              
    - 总结
      
        - 您之前的疑虑是完全正确的。**工业级的安全感不能建立在 Redis 上**。
          
        - **一人一单的保证**：
          
            - **Redis (Cache)**：负责挡住 99.9% 的正常重复流量，保护 DB 不被刷爆。
              
            - **MySQL (Unique Index)**：负责在 Redis 宕机/失效时，物理拦截那 0.1% 的逃逸流量。
              
        - **不超卖的保证**：
          
            - **Redis (DECR)**：负责高性能计数。
              
            - **MySQL (Where stock > 0)**：负责作为“法律依据”，如果没有这一句，Redis 一旦脑裂，数据库就会被扣成负数。
              
        - 加上这两条数据库层面的限制后，这个架构才是真正的**“金刚不坏”**。
          
- 理解了上面的内容，我们再来看一个魔鬼细节，为什么前端收到排队成功之后要回查防止脑裂丢单
  
    - 1. 还原“订单黑洞”现场
      
        - 假设 Redis 采用主从架构（Master-Slave），且没有开启强制同步（`WAIT` 命令）。
          
        1. **高光时刻（Master）**：
           
            - 用户请求到达 Master。
              
            - Lua 脚本执行成功：
              
                - `DECR stock` (库存 -1)
                  
                - `SADD user_list user_A` (记录用户 A 已购)
                  
                - `RPUSH outbox message_A` (消息放入发信箱)
                  
            - **响应前端**：Master 立刻给业务服务器返回“成功”，前端显示“排队中，正在处理...”。
              
        2. **至暗时刻（断电/崩溃）**：
           
            - 就在 Master 返回成功的**下一毫秒**，Master 还没来得及把这一批指令同步给 Slave，Master 挂了。
              
            - **数据状态**：这笔 `message_A` 的数据只存在于 Master 的内存里，随着断电烟消云散。
              
        3. **新皇登基（Slave -> New Master）**：
           
            - 哨兵/集群机制把 Slave 提升为 New Master。
              
            - **然而**：New Master 的内存里，库存没减，`user_list` 里没有 user_A，`outbox` 里也没有 `message_A`。
              
        4. **死寂（Relay & Consumer）**：
           
            - **Relay 服务**：连接到 New Master，去 `outbox` 捞数据。因为数据没同步过来，它捞了个空。
              
            - **RabbitMQ**：没收到消息。
              
            - **Database**：没收到订单。
              
    - 2. 用户侧的真实体验：从“排队”到“超时”
      
        - 你说得对，用户看到的就是“莫名其妙”。
          
        - **前端表现**：
          
            - 用户界面一直转圈圈：“正在为您排队...”。
              
            - 前端 JS 代码每隔 2 秒轮询一次后端接口 `GET /order/status?id=xxx`。
              
            - 后端去查 DB（或者查 Redis 缓存的订单状态），发现**查无此单**。
              
            - 后端只能返回：“等待中”或者“未找到”。
              
        - **最终结局**：
          
            - 前端轮询通常会有一个超时时间（比如 30 秒）。
              
            - 30 秒后，前端 JS 放弃轮询，弹窗提示：**“请求超时，请稍后重试”**。
              
        - **这就是你担心的“莫名其妙失败”。用户不知道是卖完了，还是系统崩了，只知道自己等了半天是个寂寞。**
          
    - 3. 但！这是一个“不幸中的万幸”
      
        - 虽然订单丢了，但在这个架构中，有一个极大的**“隐形优势”**，正是因为你把“唯一 ID”和“幂等性检查”也放在了 Redis Lua 里。
          
        - **请注意：当数据丢失时，是“一锅端”的丢失。**
          
        - `outbox` 里的消息丢了（坏事）。
          
        - **但是，****`seckill:users` 里的“用户购买记录”也丢了！**（这是好事）
          
        - **库存扣减 ****`DECR` 也丢了（库存回滚了）！**（这是好事）
          
        - **这意味着什么？**
          
        - 当用户看到“请求超时”，愤而点击**“重试”**按钮时：
          
        1. 新的请求到达 New Master。
           
        2. Lua 脚本检查 `seckill:users`：**没找到 user_A**（因为刚才那条记录也随着 Master 挂掉而丢了）。
           
        3. **结果**：脚本判断用户没买过，**允许用户再次抢购！**
           
        - **结论：**  
          如果是 Redis 脑裂导致写丢失，**用户并没有“死锁”**。他只是损失了那一次点击的先机，但他并没有被系统误判为“已购买”，他完全有机会发起第二次请求并成功买到。
          
        - 这比“Redis 记录了用户已买，但订单消息丢了”这种情况要好一万倍。如果是后者，用户想重试都会被拦截，那才是真正的死局。
          
    - 4. 这种“莫名其妙”能解决吗？（最好不解决）
      
        - 如果你无法容忍这种“前端排队 -> 超时失败”的体验，必须要求 100% 数据不丢，只有一种办法：
          
        - **Redis 牺牲性能，换取一致性（AP -> CP）**
          
        - 在 Lua 脚本执行完写操作后，不要直接返回，而是加入 `WAIT` 指令：
          
        - ```redis  
          # 写入操作...  
          RPUSH outbox ...  
          # 阻塞等待，直到至少 1 个 Slave 确认收到了数据，或者 1000ms 超时  
          WAIT 1 1000  
          ```
          
        - **效果**：只有当 Slave 也收到了数据，Master 才会告诉前端“成功”。如果 Master 挂了，前端根本收不到“成功”，直接报错。
          
        - **代价**：**巨大的性能损耗**。写操作的耗时会从 **0.x 毫秒** 暴增到 **网络RTT时间**（几毫秒到几十毫秒）。Redis 的吞吐量会暴跌。
          
    - 具体的回查方案
      
        - async function startSeckill() {  
            // 1. 锁定按钮  
            setBtn(STATUS.LOADING);  
              
            try {  
              // 2. 发起购买请求 (不需要传 ID，后端自己算)  
              // 这一步只是把请求扔进 MQ  
              await api.post('/seckill/buy', { activityId: 1001 });  
                
              // 3. 开始轮询  
              startPolling();  
                
            } catch (e) {  
              // 如果第一步就挂了（网关报错），直接让用户重试  
              setBtn(STATUS.TIMEOUT);  
            }  
          }  
            
          async function startPolling() {  
            let retryCount = 0;  
              
            const timer = setInterval(async () => {  
              // 调用只查 Redis 的接口  
              const res = await api.get('/seckill/status', { activityId: 1001 });  
                
              // 【情况 A：明确成功】  
              if (res.status === 1) {  
                clearInterval(timer);  
                setBtn(STATUS.SUCCESS);  
                redirectToOrderPage(); // 跳转  
              }  
                
              // 【情况 B：明确失败】  
              else if (res.status === -1) {  
                clearInterval(timer);  
                setBtn(STATUS.FAILED); // 按钮置灰，死心吧  
                alert(res.msg);  
              }  
                
                
                
              // 【情况 D：查无此单】(Redis 里啥都没有)  
              else if (res === 'NOT_FOUND') {  
                retryCount++;  
                // 容忍 5 秒 (即 2秒一次的话，容忍 3 次)  
                // 因为消息可能还在网络路上，或者 Master 刚挂  
                if (retryCount > 3) {  
                  clearInterval(timer);  
                  handleTimeout();  
                }else{  
                   显示排队中  
                }  
              }  
            }, 2000); // 2秒轮询一次  
          }  
            
          function handleTimeout() {  
            // 变为橙色按钮，诱导用户再次点击  
            // 【关键】再次点击调用的是 startSeckill()  
            // 也就是再次发起 POST /buy  
            setBtn(STATUS.TIMEOUT);   
            showToast("请求响应超时，请点击重试以刷新状态");  
          }
          
- 回查方案引入的接口设计，千万不要缓存穿透，不去查db
  
    - 消费成功会在redis里写一个标记为xxxfinish的key，只要这个key存在，由于我们在消费端保证了，写 Redis 的数据结构必须包含所有必要信息，让前端不需要查 DB 就能展示“抢购成功页”。
      
## 允许一人多单

### 1. 核心设计变更：ID 与 幂等键

- 在不限购场景下，我们不能再用 `SISMEMBER seckill:users` 来拦截用户，因为用户 User_A 买完 5 个，下一秒想再买 3 个是合法的。
  
- **全局唯一 ID (Request ID)**：
  
    - **生成位置**：前端点击“结算”瞬间，或者网关层。
      
    - **算法**：Snowflake（雪花算法）。
      
    - **作用**：代表“这**一次**购买行为”的唯一身份。如果你买了 5 个，这 5 个商品共享这同一个 `Request ID`。
      
- **幂等性策略**：
  
    - **Redis 层**：不再检查“人”，改为检查“这个 ID 是否处理过”。
      
    - **DB 层**：`Request ID` 依然是主键。
      
### 2. 全链路详细流程（支持自定义数量）

- 🟢 阶段 0：秒杀预热
  
    - **动作**：`SET seckill:stock:1001 10000` (存入总库存 10000)。
      
    - **注意**：这里不需要初始化 `seckill:users` 集合了，因为不限购。
      
- 🔴 阶段 1：原子扣减与 Request 级幂等（Client -> Redis）
  
    - 用户 User_A 请求：买 **5** 个，Request ID 为 **`REQ_999`**。
      
        - 动作：用户点击“立即购买”。
          
            - Step 1 生成 ID：前端 JS 生成 UUID 或 Snowflake ID，例如 REQ_XYZ_001。
              
            - Step 2 冻结按钮：将按钮置灰 disabled = true，显示 loading。
              
            - Step 3 发送请求：
              
                - POST /seckill/buy  
                  {  
                  "req_id": "REQ_XYZ_001",  // 前端生成的唯一单号  
                  "user_id": "User_A",  
                  "sku_id": "1001",  
                  "count": 5                // 这次买 5 个  
                  }
                  
            - Step 4 等待结果：前端轮询或等待 WebSocket。只有收到明确的“成功/失败/售罄”，才解冻按钮让用户下第二单。
              
        - 我们选择后端预生成的方案生成id
          
            - ✅ 方案 ：后端预生成（Token 机制）—— 推荐  
              结合了“后端生成的安全性/有序性”和“前端持有的幂等性”。通常结合秒杀的“隐藏秒杀地址”机制一起做。
              
                - 阶段一：获取令牌（Get Token）
                  
                    - 用户在详情页点击“立即购买”。
                      
                    - 前端先调用后端接口 GET /seckill/token。
                      
                    - 后端生成一个 Snowflake ID（例如 REQ_999），并将在这个 ID 中签入用户信息（签名防止篡改），返回给前端。
                      
                    - 注：这一步很快，不涉及数据库和 Redis 库存，纯计算。
                      
                - 阶段二：提交订单（Submit Order）
                  
                    - 前端拿到 REQ_999。
                      
                    - 前端调用核心秒杀接口 POST /seckill/exec，Payload：
                      
                    - {  
                      "order_id": "REQ_999",  // 刚才拿到的 ID  
                      "sku_id": 1001,  
                      "count": 5  
                      }  
                      幂等保障
                      
                    - 如果阶段二网络超时，前端带着同一个 REQ_999 重试。
                      
                    - 后端 Redis 看到 REQ_999 已存在，拦截。
                      
                - 方案 C 的额外好处：
                  
                    - 防刷单：后端可以在生成 ID 时加入加密签名（Signature），如果前端想自己瞎编一个 ID REQ_111 发过来，后端校验签名失败，直接拒绝。
                      
                    - 数据库友好：ID 是后端生成的标准的 Snowflake ID（Long 类型），MySQL 插入性能最优。
                      
    - **Lua 脚本逻辑更新**：  
      以前是检查人，现在必须检查 ID 并在 Redis 留下痕迹（防止网络重试导致多扣）。
      
        - ```lua  
          -- KEYS[1]: 库存Key (seckill:stock:1001)  
          -- KEYS[2]: 幂等记录Key (seckill:req:REQ_999) - 新增！  
          -- KEYS[3]: 消息队列Key (order:outbox)  
            
          -- ARGV[1]: 购买数量 (5)  
          -- ARGV[2]: 消息体 (JSON)  
            
          local stock_key = KEYS[1]  
          local idempotency_key = KEYS[2]  
          local outbox_key = KEYS[3]  
          local quantity = tonumber(ARGV[1])  
          local msg = ARGV[2]  
            
          -- 1. 严格幂等检查：这个 REQ_999 处理过没有？  
          -- 使用 SETNX (Set if Not Exists)  
          -- 如果返回 0，说明这个 ID 已经存在了，直接视为重复请求拦截  
          if redis.call('SETNX', idempotency_key, 1) == 0 then  
              return -1 -- 重复请求  
          end  
          -- 给幂等 Key 设置过期时间（比如 10 分钟），节省内存  
          redis.call('EXPIRE', idempotency_key, 600)  
            
          -- 2. 批量库存检查  
          local current_stock = tonumber(redis.call('GET', stock_key))  
          if current_stock < quantity then  
              -- 重点：如果库存不足，必须把刚才设置的幂等Key删掉？  
              -- 不一定。如果业务认为库存不足也算“处理了一次”，可以不删。  
              -- 但通常为了允许用户改数量重试，建议这里删掉 idempotency_key 或者不删但返回特定错误。  
              -- 简单做法：这里不删，库存不足就是不足，重试也没用。  
              return 0   
          end  
            
          -- 3. 原子扣减 (DECRBY)  
          redis.call('DECRBY', stock_key, quantity)  
            
          -- 4. 写入 Outbox  
          redis.call('LPUSH', outbox_key, msg)  
            
          return 1 -- 成功  
          ```
          
    - **结果**：Redis 内存中多了一个 `seckill:req:REQ_999` 的 Key，库存减少 5。
      
- 🔵 阶段 2：多线程可靠搬运（Redis -> Relay -> RabbitMQ）
  
    - 这一步的逻辑与之前基本一致，**Relay 不需要知道你买几个**，它只负责搬运 JSON 。
      
    - **Step A (私有拉取)**：Relay 线程 `T1` 从 Redis 拉取消息：`{ "req_id": "REQ_999", "uid": "User_A", "count": 5 }`。
      
    - **Step B (发送)**：
      
        - `channel.basicPublish`。
          
        - **关键点**：必须把 `REQ_999` 放入消息属性或作为去重依据。
          
    - **Step C (Confirm)**：等待 RabbitMQ 落盘确认。
      
    - **Step D (清理)**：删除 Redis 里的备份消息 。
      
    - **异常推演**：如果 Relay 发完 `REQ_999` 后挂了，重启后又发了一遍。RabbitMQ 里会有两条 `REQ_999` 的消息。
      
- 🟠 阶段 3：RabbitMQ 缓冲
  
    - **状态**：队列里躺着消息 `{ "req_id": "REQ_999", "count": 5 }`。如果 Relay 重试过，可能躺着两条。
      
- **🟣 阶段 4：批量入库与最终幂等（Consumer -> DB）**
  
    - 1. 表结构调整：拆除人墙，保留防重
      
        - 我们需要调整 `orders` 表的约束。
          
        - **保留** `PRIMARY KEY (order_id)`：这是为了防止**网络重试**和**MQ 消息重投**。每一个具体的“购买动作”依然是唯一的。
          
        - **删除** `UNIQUE KEY (user_id, sku_id)`：因为现在允许 User_A 对 SKU_1001 下单多次。
          
        - ```sql  
          CREATE TABLE `orders` (  
            `order_id` VARCHAR(64) NOT NULL, -- Request ID (Snowflake)  
            `user_id` BIGINT NOT NULL,  
            `sku_id` INT NOT NULL,  
            `count` INT NOT NULL, -- 本次买几个  
            `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP,  
              
            PRIMARY KEY (`order_id`) -- 【兜底一】Request 级幂等，拦截 MQ 重投  
            -- 注意：这里没有针对 user_id 的唯一索引了  
          ) ENGINE=InnoDB;  
          ```
          
    - 2. Consumer 消费逻辑：乐观锁扣减
      
        - 当 Consumer 收到消息 `{req: "REQ_005", user: "A", count: 2}`（User A 想买 2 个）时，数据库的操作流程如下：
          
        - Step 1: 开启事务
          
            - `BEGIN TRANSACTION;`
              
        - Step 2: 幂等性检查 (Request Level)
          
            - 即使允许一人多单，也不能让**同一个请求**执行两次。
              
            - ```sql  
              INSERT INTO orders (order_id, user_id, sku_id, count)   
              VALUES ('REQ_005', 'User_A', 1001, 2);  
              ```
              
            - **兜底逻辑**：
              
                - 如果报错 `Duplicate Key (PRIMARY)`：说明是 **MQ 重复投递**。
                  
                - **动作**：回滚事务，直接 ACK。
                  
        - Step 3: 核心库存兜底 (Stock Level)
          
            - 这是防止 Redis 脑裂导致超卖的**唯一防线**。利用 SQL 的原子性进行扣减。
              
            - ```sql  
              -- 【兜底二】防止超卖的核心 SQL  
              UPDATE products   
              SET stock = stock - 2  -- 扣减本次购买的数量  
              WHERE id = 1001   
                AND stock >= 2;      -- 【关键】必须确保 剩余库存 >= 本次购买量  
              ```
              
            - **正常情况**：库存充足（例如库存是 10，10 >= 2）。
              
                - 返回 **Affected Rows = 1**。
                  
                - **动作**：`COMMIT` 事务，ACK。
                  
            - **异常情况 (Redis 脑裂/数据不一致)**：
              
                - 场景：Redis 以为还有 5 个，放了请求进来。但数据库其实只剩 1 个了。
                  
                - 执行：`UPDATE ... WHERE stock >= 2`。因为 `1 >= 2` 不成立。
                  
                - 结果：返回 **Affected Rows = 0**。
                  
                - **兜底动作**：
                  
                    1. **回滚事务** (`ROLLBACK`)：撤销刚才 Step 2 插入的订单记录（因为没货了，这单不能成）。
                       
                    2. **ACK**：确认消息消费（虽然业务失败了，但处理过程结束了）。
                       
                    3. **告警/通知**：记录日志“Redis 库存与 DB 不一致，触发兜底拦截”。
                       
        - **Step 4: **
          
            - **回写redis：**然后回写redis某个键为finish，库存减没减这单都成了，要是设置redis失败了可以象征性重试几次，最差也就被认为not found呗，记得设置超时时间为被认为Not found的限制时间
              
            - 隐患（千万别回表）： 这是绝对禁止的。 在秒杀结束后的几秒内，可能有 10 万人同时抢到了商品，同时轮询出了 SUCCESS 状态。 如果这 10 万人同时发起 SELECT * FROM orders WHERE id = ...，数据库必挂无疑。哪怕是查主键，瞬间 10w QPS 的并发也是 MySQL 扛不住的。
              
                - ✅ 修正建议：Redis 结果即真理 Consumer 回写 Redis 的数据结构必须包含所有必要信息，让前端不需要查 DB 就能展示“抢购成功页”。
                  
            - 就算真not found了，也让用户去“我的订单”里找，在那里查db
              
    - 3. 完整防御逻辑总结表
      
        - **消息重投 / 网络重试**
          
            - Lua `SETNX req_id`
              
            - **有效**
              
            - `PRIMARY KEY (order_id)` 抛出主键冲突。
              
        - **单人多单 (业务允许)**
          
            - **放行**
              
            - **放行**
              
            - 移除了针对 user_id 的唯一索引。
              
        - **超卖 (Redis 脑裂)**
          
            - Lua `DECR`
              
            - **有效 (拦截)**
              
            - `UPDATE ... WHERE stock >= count` 返回 0 行。
              
    - 4. 为什么这样是安全的？
      
        - 哪怕 Redis 挂了，或者 Redis 集群脑裂导致它放进来了 10000 个购买请求（实际库存只有 10 个）。
          
        1. 这 10000 个请求都会进入 MQ。
           
        2. Consumer 会尝试执行 10000 次数据库事务。
           
        3. 前 5 次事务（假设每人买2个），DB 库存足够，`UPDATE` 成功，入库成功。
           
        4. **第 6 次到第 10000 次事务**：
           
            - `INSERT orders` 会成功（因为 ID 不同）。
              
            - 但在执行 `UPDATE products` 时，因为 `stock` 已经是 0 了，`WHERE stock >= 2` 条件永远无法满足。
              
            - 数据库返回 `0` 行受影响。
              
            - 代码逻辑触发 `ROLLBACK`。
              
            - 订单记录被回滚消失。
              
        - **结论**：只要数据库的 `UPDATE ... WHERE stock >= count` 这行代码在，无论 Redis 发生什么灾难级故障，**绝对不会超卖**。
          
- 魔鬼细节，排队之后回查防止脑裂丢单
  
    - 前端解冻与反馈  
      这个环节回答了你关于“前端按钮冻结”的闭环。
      
        - 用户点击后：按钮变灰，开始轮询 /order/status?req_id=REQ_XYZ_001。
          
        - 后端查询：
          
            - 查 Redis 缓存？没有（可能还没处理完）。
              
            - 查 DB orders 表？有了！
              
        - 返回结果：
          
            - 后端返回：{ status: "SUCCESS" }。
              
        - 前端动作：
          
            - 提示：“抢购成功！”
              
        - 解冻按钮：将按钮 disabled = false。
          
        - 用户现在可以点击第二次，生成新的 REQ_XYZ_002，开始下一轮循环。
          
    - **在“一人多单”场景下，由于 ID 是前端生成的，这个过程会发生微妙的变化，我们来推演一下：**
      
        - **“脑裂（Split-Brain）”和“主从切换数据丢失”是 Redis 架构层面的物理特性**，无论你的 Lua 脚本里写的是“一人一单”还是“一人多单”，只要你用了 Redis 的异步复制（默认模式），这个风险就如同地心引力一样存在。
          
        - 1. 灾难复现：REQ_001 的消失
          
            - 假设用户 User_A 此时点了下单，前端生成了 `REQ_001`。
              
            1. **写入成功（Master）**：
               
                - 前端发送 `REQ_001`。
                  
                - Redis Master 执行 Lua：
                  
                    - `SETNX seckill:req:REQ_001` (成功，记录了 ID)
                      
                    - `DECR stock` (库存 100 -> 99)
                      
                    - `LPUSH outbox` (放入消息)
                      
                - Master 给后端返回“1”（成功），后端告诉前端“排队中”。
                  
            2. **同步失败与崩溃**：
               
                - Master 还没来得及把 `REQ_001` 和 `库存=99` 同步给 Slave，就断电了。
                  
            3. **新主上位（New Master）**：
               
                - Slave 变成新 Master。
                  
                - **状态回滚**：新 Master 里没有 `REQ_001`，且库存依然是 **100**（注意这里！库存也“自动回滚”了，因为扣减操作丢了）。
                  
        - 2. 后果分析：这对“一人多单”意味着什么？
          
            - 这里有一个非常有趣的现象：**在“一人多单”场景下，这种“丢失”反而比“一人一单”更“安全”且容易恢复。**
              
            - 我们来看三种可能的后果：
              
            - 情况 A：前端超时后，用户手动重试（生成新 ID）
              
                - 用户看到“请求超时”，很不爽，又点了一次。
                  
                - 前端生成新 ID `REQ_002`。
                  
                - 发送给新 Master。
                  
                - **结果**：**成功**。
                  
                - **库存**：库存从 100 变成 99。
                  
                - **总结**：用户感觉只是第一次网卡了，第二次买到了。对于系统来说，没毛病。
                  
            - 情况 B：前端代码自动重试（使用旧 ID）
              
                - 前端代码写得好，发现超时了，自动发起重试。
                  
                - **关键点**：重试时带着**原来的 ID ****`REQ_001`**。
                  
                - 发送给新 Master。
                  
                - **Lua 脚本检查**：
                  
                    - `SETNX seckill:req:REQ_001`？
                      
                    - 因为刚才那条记录丢了，所以这里**返回 1 (成功)**！
                      
                - **结果**：**成功**。
                  
                - **总结**：这对用户是完全透明的！用户甚至不知道中间发生过 Redis 宕机，他只知道自己买到了。
                  
                    - *注：如果是“一人一单”场景，如果用户名单 **`Set` 没丢但订单丢了，重试就会报错“你已买过”。但在“一人多单”里，只要 ID 丢了，重传 ID 就能完美补救。*
                      
            - 情况 C：用户放弃了
              
                - 用户看超时了，骂了一句“垃圾系统”，关掉了 App。
                  
                - **结果**：
                  
                    - 订单 `REQ_001` 永久丢失。
                      
                    - **库存**：因为 `DECR` 操作也丢了，库存停留在 100。
                      
                    - **总结**：系统没有任何坏账。没有发生“库存扣了但没生成订单”的情况（少卖），也没有发生“生成了订单但没扣库存”的情况（超卖）。库存和订单完美守恒（都丢了）。
                      
        - 3. 核心结论
          
            - 在“一人多单” + “前端生成 ID” + “Redis 异步复制”的架构下：
              
            1. **订单确实会丢**：脑裂时，处于窗口期的数据 100% 会丢。
               
            2. **但库存是安全的**：因为 Redis 里的库存扣减操作和消息入队操作是原子性一起丢的。不会出现库存少了、订单却没下的情况。
               
            3. **重试机制极其顺滑**：
               
                - 正因为“幂等 Key (`REQ_001`)”也一起丢了，所以**前端带着同一个 ID 重试竟然能成功！**
                  
                - 这是不幸中的万幸。故障发生后，新 Master 就像失忆了一样，把它当成一个全新的请求处理，这反而保证了业务的连续性。
                  
        - 总结
          
            - 在这个场景下，**脑裂导致的丢失不可怕**。
              
            - **对系统**：库存没乱，数据没脏。
              
            - **对用户**：要么超时失败（放弃），要么重试成功（系统以为是第一次）。
              
            - 你唯一需要确保的是：**前端（或网关）要有合理的超时重试机制**。只要前端敢重试，后端就能接得住，仿佛什么都没发生过一样。
              
### 一人多单的前端设计方案

- 在这个场景下，用户的行为模式是 **“循环购买”**：点一次 -> 买一个 -> 成功 -> 再点一次 -> 再买一个。
  
- 设计的核心目标是：**保证每一次点击的原子性（不因网络抖动买多），同时在 Redis 脑裂丢失数据时提供“无痛”的重试恢复。**
  
- 🟢 核心交互模型：循环状态机 (The Loop Protocol)
  
    - 不同于“一人一单”的单行道，这里是一个闭环。
      
    - 1. 核心变量定义 (Vue/React State)
      
        - 前端组件内部必须维护以下核心状态：
          
        - ```javascript  
          state = {  
            status: 'IDLE',      // 枚举: IDLE | PENDING | SUCCESS | TIMEOUT | FAILED  
            currentReqId: null,  // 关键！当前这一笔交易的身份证  
            btnText: '立即抢购',  
            isBtnDisabled: false  
          }  
          ```
          
    - 2. 按钮的五种形态
      
        - **IDLE (空闲)**
          
            - 🔴 **立即抢购**
              
            - 高亮/主色
              
            - **可点击**
              
            - 页面加载完成，或上一单已结束后。
              
        - **PENDING (处理)**
          
            - ⏳ **排队中...**
              
            - 灰色/Loading
              
            - **禁用**
              
            - 点击购买后，轮询未出结果前。
              
        - **TIMEOUT (迷茫)**
          
            - ⚠️ **超时，点此重试**
              
            - 橙色/警告色
              
            - **可点击**
              
            - 轮询 10秒+ 查不到数据 (脑裂/丢包)。
              
        - **SUCCESS (成功)**
          
            - ✅ **抢购成功！再来一单**
              
            - 绿色 -> 红色
              
            - **可点击**
              
            - 收到后端 `SUCCESS` 信号。
              
        - **FAILED (失败)**
          
            - ❌ **库存不足**
              
            - 黑色/置灰
              
            - **禁用**
              
            - 收到后端 `FAILED` 信号。
              
- 🔵 全链路交互流程详解
  
    - 1. 阶段一：发起购买 (Start)
      
        - 用户点击“立即抢购”。
          
        - **生成 ID**：前端生成一个新的 UUID/Snowflake，赋值给 `currentReqId`。
          
            - *注意：这是“这一个订单”的 ID，不是用户的 ID。*
              
        - **锁定 UI**：
          
            - 状态变更为 `PENDING`。
              
            - 按钮变灰，显示“排队中...”。
              
            - **禁止用户再次点击**（防止手抖发了两个请求）。
              
        - **发送请求**：`POST /buy { req_id: currentReqId, count: 1 }`。
          
    - 2. 阶段二：轮询与等待 (The Black Box)
      
        - 前端启动定时器，每 1.5秒 调用 `GET /order/status?req_id=currentReqId`。
          
        - **情况 A：正常排队**
          
            - 后端返回：`{ status: "QUEUED" }`
              
            - 前端动作：继续轮询，保持 Loading 状态。
              
        - **情况 B：脑裂/丢数据风险 (关键)**
          
            - 后端返回：`404 Not Found` (Redis 没查到这单子)。
              
            - **前端策略**：
              
                - **前 5 秒**：假装没事，继续轮询（可能是消息刚发出去，Redis 还没写入）。
                  
                - **超过 5 秒**：判定为**“潜在丢失”**，进入**阶段三（异常处理）**。
                  
    - 3. 阶段三：异常处理 (TIMEOUT 态)
      
        - 这是为了对抗 Redis 脑裂导致写丢失的设计。
          
        - **现象**：轮询超时，后端一直说“没见到这个单号”。
          
        - **UI 变化**：
          
            - 按钮变更为 **橙色**。
              
            - 文案变更为 **“请求超时，点此重试”**。
              
            - 按钮 **恢复可点击**。
              
        - **用户动作**：用户点击“重试”。
          
        - **关键逻辑（必须死记）**：
          
            - **不要生成新 ID！**
              
            - **复用 ****`currentReqId`** 再次发送 `POST /buy`。
              
            - *原理*：
              
                - 如果 Redis 真的丢了数据，这次请求会补上，用户无感知。
                  
                - 如果 Redis 没丢（只是网卡了），这次请求会被 Lua 脚本拦截（幂等），后端返回“成功”，用户依然无感知。
                  
    - 4. 阶段四：成功与循环 (Loop)
      
        - 后端返回 `{ status: "SUCCESS" }`。
          
        - **UI 变化**：
          
            - 按钮短暂变绿：**“抢购成功！”** (持续 1-2 秒)。
              
            - 随后立刻变回红色：**“再来一单”**。
              
            - **状态重置**：`status = IDLE`。
              
        - **销毁 ID**：
          
            - 将 `currentReqId` 置为 `null`。
              
            - **意味着**：用户下一次点击，将生成全新的 ID，开始下一轮抢购。
              
- 🟡 代码逻辑复刻 (给前端开发的伪代码)
  
    - ```javascript  
      // 全局变量，记录当前正在处理的单号  
      let currentTransactionId = null;  
        
      async function onBtnClick() {  
          
        // 1. 【ID 策略】  
        // 如果是“超时重试”场景，currentTransactionId 已经有值了，复用它！  
        // 如果是“新开一单”场景，生成新的。  
        if (!currentTransactionId) {  
          currentTransactionId = generateUUID();   
        }  
        
        // 2. 【锁定 UI】  
        updateBtn({ text: "排队中...", disabled: true, style: "gray" });  
        
        try {  
          // 3. 【发送请求】始终携带同一个 ID  
          await api.post('/seckill/buy', {   
            req_id: currentTransactionId,   
            sku: 'iphone',  
            count: 1   
          });  
        
          // 4. 【启动轮询】  
          pollStatus(currentTransactionId);  
        
        } catch (err) {  
          handleNetworkError();  
        }  
      }  
        
      async function pollStatus(reqId) {  
        const result = await api.get(`/seckill/status?req_id=${reqId}`);  
        
        // 场景 A: 成功  
        if (result.status === 'SUCCESS') {  
          showToast("抢到了！");  
          // 【关键】清空 ID，允许下一单生成新 ID  
          currentTransactionId = null;   
          updateBtn({ text: "再来一单", disabled: false, style: "red" });  
          return;  
        }  
        
        // 场景 B: 失败 (没货了)  
        if (result.status === 'FAILED') {  
          updateBtn({ text: "库存不足", disabled: true, style: "black" });  
          return;  
        }  
        
        // 场景 C: 还没查到 (可能在排队，也可能脑裂丢了)  
        if (result.status === 'NOT_FOUND') {  
          if (pollTime > 5000) { // 超过 5 秒还没查到  
            // 【关键】进入超时态，保留 ID，让用户手动重试  
            updateBtn({ text: "超时，点此重试", disabled: false, style: "orange" });  
            return;   
          }  
          // 继续轮询  
          setTimeout(() => pollStatus(reqId), 1000);  
        }  
      }  
      ```
      
- 🟣 总结：为什么这样设计最稳？
  
    1. **防手抖（本地锁）**：`PENDING` 状态锁死按钮，防止用户想买 1 个结果点了 2 次，导致买了 2 个。
       
    2. **防脑裂（ID 复用）**：`TIMEOUT` 状态下，用户点击“重试”时，使用的是**旧 ID**。这保证了如果之前的请求其实成功了（只是响应丢了），Redis 会拦截这次重试；如果之前的请求丢了，这次会补上。
       
    3. **允许复购（ID 销毁）**：一旦明确 `SUCCESS`，立即销毁 ID。用户下一次点击，就是一笔全新的交易，不受幂等性限制，完美契合“一人多单”。
       
    - 这就是**“铁打的 ID（重试时），流水的 ID（成功后）”**策略。
      
### 回查方案引入的接口设计，千万不要缓存穿透，不去查db

- async function pollStatus(reqId) {  
    const result = await api.get(`/seckill/status?req_id=${req_id} `)  
  }
  
- 消费成功会在redis里写一个标记为xxxfinish的key，只要这个key存在，由于我们在消费端保证了，写 Redis 的数据结构必须包含所有必要信息，让前端不需要查 DB 就能展示“抢购成功页”。
  
### 3. 极端场景问答 (Q&A)

- **Q1: 如果 User_A 先买 5 个，过了一会又买 3 个，会被拦截吗？**
  
    - **Client 端**：生成了两个 ID，分别是 `REQ_999` (5个) 和 `REQ_1000` (3个)。
      
    - **Redis Lua**：
      
        - 检查 `seckill:req:REQ_999` -> 不存在 -> 通过 -> 扣5。
          
        - 检查 `seckill:req:REQ_1000` -> 不存在 -> 通过 -> 扣3。
          
    - **DB 端**：插入两条数据，主键不同，互不冲突。
      
    - **结果**：User_A 成功买了两次，共 8 个。**符合预期**。
      
- **Q3: 为什么 Redis 要设置过期时间 ****`EXPIRE`？**
  
    - 如果不删，Redis 里的 `seckill:req:xxx` 键会越来越多，内存会爆。
      
    - 设置 10 分钟或 1 小时即可。因为秒杀高峰过后，用户不太可能再拿几小时前的旧 ID 来重试。即便来了，数据库的主键也能做最后一道防守。
      
### 总结

- 在**“一人多单 + 自定义数量”**场景下：
  
1. **ID 设计**：ID 代表“请求”而非“用户”。
   
2. **Redis 职责**：利用 `SETNX request_id` 防止**瞬时网络重试**造成的重复扣减；利用 `DECRBY` 处理批量扣减。
   
3. **DB 职责**：利用 `PRIMARY KEY (request_id)` 防止**消息队列重投**造成的重复入库。
   
## 累计限购

### 以前的逻辑是“要么没买，要么买了”（0 或 1），现在的逻辑变成了“你当前已经买了 X 个，还能买 5-X 个”。

### 我们需要引入一个“用户配额计数器 (User Quota Counter)”。整个设计的核心在于：**在 Redis Lua 脚本中同时维护“全局库存”和“个人限购数”的原子性**。

### 以下是针对 **“总限购 5 个，允许分单购买”** 的详细设计方案。

### 1. 核心设计变化

- **全局唯一 ID (Request ID)**：
  
    - 依然是 **Snowflake**。它代表“这一笔订单”（比如买 2 个）。
      
    - **作用**：防止同一笔订单因为网络超时重试，导致用户的配额被扣两次。
      
    - 我们选择后端预生成的方案生成id
      
        - ✅ 方案 ：后端预生成（Token 机制）—— 推荐  
          结合了“后端生成的安全性/有序性”和“前端持有的幂等性”。通常结合秒杀的“隐藏秒杀地址”机制一起做。
          
            - 阶段一：获取令牌（Get Token）
              
                - 用户在详情页点击“立即购买”。
                  
                - 前端先调用后端接口 GET /seckill/token。
                  
                - 后端生成一个 Snowflake ID（例如 REQ_999），并将在这个 ID 中签入用户信息（签名防止篡改），返回给前端。
                  
                - 注：这一步很快，不涉及数据库和 Redis 库存，纯计算。
                  
            - 阶段二：提交订单（Submit Order）
              
                - 前端拿到 REQ_999。
                  
                - 前端调用核心秒杀接口 POST /seckill/exec，Payload：
                  
                - {  
                  "order_id": "REQ_999",  // 刚才拿到的 ID  
                  "sku_id": 1001,  
                  "count": 5  
                  }  
                  幂等保障
                  
                - 如果阶段二网络超时，前端带着同一个 REQ_999 重试。
                  
                - 后端 Redis 看到 REQ_999 已存在，拦截。
                  
            - 方案 C 的额外好处：
              
                - 防刷单：后端可以在生成 ID 时加入加密签名（Signature），如果前端想自己瞎编一个 ID REQ_111 发过来，后端校验签名失败，直接拒绝。
                  
                - 数据库友好：ID 是后端生成的标准的 Snowflake ID（Long 类型），MySQL 插入性能最优。
                  
- **Redis 数据结构新增**：
  
    - `seckill:stock:1001` (String): 全局总库存（例如 10000）。
      
    - `seckill:quota:1001:{user_id}` (String/Hash): **新增！** 记录该用户当前已抢到的总数。
      
    - `seckill:req:{req_id}` (String): 记录处理过的请求 ID（防网络重试）。
      
### 2. 全链路流程详解

- 假设 User_A 已经买过 2 个了，现在发请求想**再买 3 个**（请求 ID: `REQ_002`）。
  
- 🟢 阶段 1：Redis Lua 的“三重原子检查” (Client -> Redis)
  
    - 这是最关键的一步。Lua 脚本必须像通过“三道闸门”一样处理请求。
      
    - **Lua 脚本逻辑（伪代码）：**
      
    - ```lua  
      -- KEYS[1]: 全局库存 Key (seckill:stock:1001)  
      -- KEYS[2]: 用户配额 Key (seckill:quota:1001:User_A)  
      -- KEYS[3]: 请求幂等 Key (seckill:req:REQ_002)  
      -- KEYS[4]: 消息队列 Key (order:outbox)  
        
      -- ARGV[1]: 本次购买数量 (3)  
      -- ARGV[2]: 总限购数量 (5)  
      -- ARGV[3]: 消息体 JSON  
        
      local stock_key = KEYS[1]  
      local quota_key = KEYS[2]  
      local req_key   = KEYS[3]  
      local outbox    = KEYS[4]  
        
      local buy_num   = tonumber(ARGV[1]) -- 3  
      local limit_num = tonumber(ARGV[2]) -- 5  
      local msg       = ARGV[3]  
        
      -- 1. 【第一道闸】请求幂等性检查 (防止网络重试)  
      -- 如果这个 REQ_002 已经处理过，直接返回成功 (幂等)  
      if redis.call('EXISTS', req_key) == 1 then  
          return 1   
      end  
        
      -- 2. 【第二道闸】个人累积限购检查  
      -- 获取用户当前已买数量，如果没有则为 0  
      local current_bought = tonumber(redis.call('GET', quota_key) or "0")  
        
      -- 如果 (已买 + 想买) > 限购数，拒绝  
      if (current_bought + buy_num) > limit_num then  
          return -2 -- 超过限购配额  
      end  
        
      -- 3. 【第三道闸】全局库存检查  
      local global_stock = tonumber(redis.call('GET', stock_key) or "0")  
      if global_stock < buy_num then  
          return 0 -- 库存不足  
      end  
        
      -- 4. 【执行扣减与记录】(原子操作)  
      redis.call('DECRBY', stock_key, buy_num)           -- 扣全局库存  
      redis.call('INCRBY', quota_key, buy_num)           -- 加个人配额 (关键变化)  
      redis.call('SETEX', req_key, 600, 1)               -- 标记请求已处理 (10分钟过期)  
      redis.call('LPUSH', outbox, msg)                   -- 写入 MQ 发货  
        
      return 1 -- 成功  
      ```
      
    - **执行结果推演：**
      
    1. Redis 查到 User_A 之前买过 2 个。
       
    2. 计算 `2 + 3 = 5`，满足 `<= 5` 的条件。
       
    3. Redis 扣减库存，User_A 的配额变为 5。
       
    4. User_A 下次如果想再买 1 个，计算 `5 + 1 = 6`，直接返回 -2 拒绝。
       
- 🔵 阶段 2 & 3：搬运与 MQ (不变)
  
    - Relay 服务和 RabbitMQ 不需要任何改动。它们只负责把“User_A 买 3 个”的消息搬运给消费者。
      
- 🔵 阶段 4. 终极版代码流程 (Consumer -> DB)
  
    - 我们采用 **“幂等先行 + SQL 合并”** 的策略。
      
    - 表结构 (保持不变)
      
        - ```sql  
          CREATE TABLE `user_quota` (  
            `user_id` BIGINT NOT NULL,  
            `sku_id` INT NOT NULL,  
            `owned_count` INT DEFAULT 0,  
            PRIMARY KEY (`user_id`, `sku_id`)  
          ) ENGINE=InnoDB;  
            
          CREATE TABLE `orders` (  
            `order_id` VARCHAR(64) NOT NULL, -- Request ID  
            ...  
            PRIMARY KEY (`order_id`)  
          ) ENGINE=InnoDB;  
          ```
          
    - 消费者执行逻辑 (伪代码 + SQL)
      
        - Consumer 收到消息：`{req: "REQ_01", user: "A", count: 2}`
          
        - **Step 1: 开启事务**  
          `BEGIN;`
          
        - **Step 2: 幂等性“安检” (第一道门)**  
          先尝试插入订单。如果这是个重试请求，在这里就应该停下。
          
        - ```sql  
          INSERT INTO orders (order_id, user_id, sku_id, count)   
          VALUES ('REQ_01', 'User_A', 1001, 2);  
          ```
          
        - **异常处理**：
          
            - 如果报 `Duplicate entry ... PRIMARY`：说明是**重复消费**。
              
            - **动作**：`ROLLBACK` -> **直接 ACK**（视为成功）。不要往下走了。
              
        - **Step 3: 累计限购检查 + 扣减 (第二道门)**  
          这里我们使用 `INSERT ON DUPLICATE KEY UPDATE` 的高级用法，将“初始化”和“更新”合并为一条 SQL，原子性更强。
          
        - ```sql  
          -- 尝试插入或更新配额  
          INSERT INTO user_quota (user_id, sku_id, owned_count)   
          VALUES ('User_A', 1001, 2) -- 2 是本次想买的数量(作为初始值)  
          ON DUPLICATE KEY UPDATE   
              -- 【关键逻辑】如果 (已有 + 本次) <= 5，则更新；否则保持原样  
              owned_count = IF(owned_count + 2 <= 5, owned_count + 2, owned_count);  
          ```
          
        - **检查结果 (Affected Rows)**：
          
            - **1**：新用户，插入成功（0 -> 2）。**通过**。
              
            - **2**：老用户，更新成功（例如 3 -> 5）。**通过**。
              
            - **0**：老用户，更新失败（例如 4 + 2 > 5，值未变）。**拦截！**
              
        - **动作**：如果是 **0**，说明超限。`ROLLBACK` -> **ACK** (业务失败)。
          
        - **Step 4: 全局库存扣减 (第三道门)**
          
        - ```sql  
          UPDATE products   
          SET stock = stock - 2   
          WHERE id = 1001 AND stock >= 2;  
          ```
          
        - **检查结果**：
          
            - **1**：库存充足。**通过**。
              
            - **0**：库存不足（Redis 脑裂了）。**拦截！**
              
        - **动作**：如果是 **0**，说明没货。`ROLLBACK` -> **ACK**。
          
        - **Step 5: 提交事务**  
          `COMMIT;` -> **ACK**。
          
        - **Step 6: 回写redis：**
          
            - **回写redis：**然后回写redis某个键为finish，库存减没减这单都成了，要是设置redis失败了可以象征性重试几次，最差也就被认为not found呗，记得设置超时时间为被认为Not found的限制时间
              
            - 隐患（千万别回表）： 这是绝对禁止的。 在秒杀结束后的几秒内，可能有 10 万人同时抢到了商品，同时轮询出了 SUCCESS 状态。 如果这 10 万人同时发起 SELECT * FROM orders WHERE id = ...，数据库必挂无疑。哪怕是查主键，瞬间 10w QPS 的并发也是 MySQL 扛不住的。
              
                - ✅ 修正建议：Redis 结果即真理 Consumer 回写 Redis 的数据结构必须包含所有必要信息，让前端不需要查 DB 就能展示“抢购成功页”。
                  
            - 就算真not found了，也让用户去“我的订单”里找，在那里查db
              
- 场景推演（新逻辑验证）
  
    - 假设：限购 5 个。
      
    - 场景 A：正常购买（已买 3，买 2）
      
        1. `INSERT orders`: 成功。
           
        2. `INSERT quota ... UPDATE`:
           
            - 原值为 3。条件 `3+2 <= 5` 成立。
              
            - 更新为 5。
              
            - **MySQL 返回：2** (Row updated)。-> **通过**。
              
        3. `UPDATE stock`: 返回 1。-> **通过**。
           
        4. `COMMIT`: **成交**。
           
    - 场景 B：超限购买（已买 4，买 2）
      
        1. `INSERT orders`: 成功。
           
        2. `INSERT quota ... UPDATE`:
           
            - 原值为 4。条件 `4+2 <= 5` **不成立**。
              
            - 执行 `owned_count = owned_count` (保持 4)。
              
            - **MySQL 返回：0** (No change)。-> **触发失败**。
              
        3. `ROLLBACK`: 刚才插入的 `orders` 记录被撤销。
           
        4. 结果：**下单失败**。符合预期。
           
    - 场景 C：网络重试（重复请求）
      
        - *背景：场景 A 实际上已经成功了，但 ACK 丢了，MQ 重发消息。*
          
        1. `INSERT orders`:
           
            - **MySQL 报错**：`Duplicate entry 'REQ_01'`。
              
        2. `Catch Exception`: 捕获到主键冲突。
           
        3. `ROLLBACK`: (虽然没做啥，习惯性回滚)。
           
        4. `ACK`: 告诉 MQ “我处理完了”（其实是幂等成功）。
           
        5. 结果：**用户看到“购买成功”**。
           
            - *注：如果是旧逻辑，这里会去检查 Quota，发现已买 5 个，无法再加 2 个，报错“限购已满”，用户体验崩塌。*
              
### 3. 常见“坑”与解决方案

- Q1: 如果 Redis 的用户配额数据丢了怎么办？(Redis 崩溃/重启)
  
    - **场景**：User_A 在 Redis 里记录买了 5 个。Redis 挂了，数据没持久化（或 AOF 没来得及刷盘）。Redis 重启后，User_A 的配额 Key 消失了。  
      **后果**：User_A 可以再买 5 个。总共买了 10 个（超限）。  
      **解决方案**：
      
    - **方案 A (容忍)**：对于秒杀活动，稍微超限几个通常是可以接受的（只要总库存不超卖）。这是最高性能做法。
      
    - **方案 B (预热加载)**：秒杀开始前把 VIP 用户的购买记录预热进 Redis（不现实）。
      
    - **方案 C (DB 校验)**：在 Lua 里不做处理，但在 Consumer 入库时，利用数据库 sum 校验（`SELECT SUM(count) FROM orders WHERE user_id = A`）。如果超限，回滚并**不发货**（不仅要回滚 DB，还要异步补偿 Redis 库存）。—— **这太复杂，不推荐用于高并发秒杀，建议选方案 A。**
      
- Q2: 为什么一定要先检查 Request ID (幂等)，再检查配额？
  
    - **错误顺序演示**：
      
    1. **先加配额**：User_A (已买2个) 发送请求 `REQ_002` (买3个)。Lua 先执行 `INCRBY`，配额变成 5。
       
    2. **网络中断**：客户端没收到响应，以为失败了。
       
    3. **重试**：客户端再次发送 `REQ_002`。
       
    4. **再次加配额**：Lua 执行 `INCRBY`，配额变成 5+3=8。
       
    5. **判断**：8 > 5，Lua 报错“配额已满”。  
       **结果**：User_A 明明只买了 5 个，却因为一次重试，导致系统认为他超限了，这单交易失败了（也就是配额被“吞”了）。  
       **正确顺序**：先检查 `seckill:req:REQ_002` 是否存在。如果存在，说明刚才加过配额了，直接返回成功。
       
### 总结图表

- (empty)
  
    - **1. 幂等**
      
        - `req_id`
          
        - 之前处理过这个 ID 吗？
          
        - (无，直接通过)
          
        - **直接返回成功** (不扣库存，不加配额)
          
    - **2. 配额**
      
        - `user_quota`
          
        - 已买 + 本次 > 5 ?
          
        - **返回 "限购已满"**
          
        - 内存中准备加数
          
    - **3. 库存**
      
        - `global_stock`
          
        - 剩余库存 < 本次 ?
          
        - **返回 "已抢光"**
          
        - 内存中准备扣数
          
    - **4. 执行**
      
        - Redis Write
          
        - **原子执行**：<br>1. 记录 req_id<br>2. 增加 quota<br>3. 扣减 stock<br>4. 推送 Outbox
          
        - -
          
        - 返回 1
          
- 通过这个设计，您可以在支持**“高并发”**的同时，完美支持**“累积限购”**和**“分单购买”**，且数据严格一致。
  
### 累计限购的前端交互设计，还是反复查询id的订单状态

- 在“累计限购 + 脑裂风险”的背景下，按钮不仅仅是一个开关，它是**用户与“薛定谔的订单”之间唯一的沟通桥梁**。
  
    - ```javascript  
      // 全局变量，记录当前正在处理的单号  
      let currentTransactionId = null;  
        
      async function onBtnClick() {  
          
        // 1. 【ID 策略】  
        // 如果是“超时重试”场景，currentTransactionId 已经有值了，复用它！  
        // 如果是“新开一单”场景，生成新的。  
        if (!currentTransactionId) {  
          currentTransactionId = generateUUID();   
        }  
        
        // 2. 【锁定 UI】  
        updateBtn({ text: "排队中...", disabled: true, style: "gray" });  
        
        try {  
          // 3. 【发送请求】始终携带同一个 ID  
          await api.post('/seckill/buy', {   
            req_id: currentTransactionId,   
            sku: 'iphone',  
            count: 1   
          });  
        
          // 4. 【启动轮询】  
          pollStatus(currentTransactionId);  
        
        } catch (err) {  
          handleNetworkError();  
        }  
      }  
        
      async function pollStatus(reqId) {  
        const result = await api.get(`/seckill/status?req_id=${reqId}`);  
        
        // 场景 A: 成功  
        if (result.status === 'SUCCESS') {  
          showToast("抢到了！");  
          // 【关键】清空 ID，允许下一单生成新 ID  
          currentTransactionId = null;   
          updateBtn({ text: "再来一单", disabled: false, style: "red" });  
          return;  
        }  
        
        // 场景 B: 失败 (没货了)  
        if (result.status === 'FAILED') {  
          updateBtn({ text: "库存不足", disabled: true, style: "black" });  
          return;  
        }  
        
        // 场景 C: 还没查到 (可能在排队，也可能脑裂丢了)  
        if (result.status === 'NOT_FOUND') {  
          if (pollTime > 5000) { // 超过 5 秒还没查到  
            // 【关键】进入超时态，保留 ID，让用户手动重试  
            updateBtn({ text: "超时，点此重试", disabled: false, style: "orange" });  
            return;   
          }  
          // 继续轮询  
          setTimeout(() => pollStatus(reqId), 1000);  
        }  
      }  
      ```
      
- 我们将按钮的状态分为 **UI 表现 (Visual)**、**交互能力 (Interactive)** 和 **背后的逻辑 (Logic)** 三个维度来设计。
  
- 以下是针对不同后端返回状态的**按钮全生命周期设计表**：
  
- 🟢 状态一：PENDING (正常排队中)
  
    - **触发条件**：后端返回status: "NOT_FOUND"` (且在 5 秒内)。
      
    - **场景**：Redis 正常，消息在队列里；或者消息刚发出去还没写入 Redis（短暂延迟）。
      
    - (empty)
      
        - **按钮文案**
          
            - ⏳ **“正在排队...”** 或 **“处理中...”**
              
        - **样式 (Style)**
          
            - **灰色 (Gray)** / 半透明
              
        - **图标**
          
            - 必须带有一个 **Loading 转圈动画** (安抚用户焦虑)
              
        - **交互 (Click)**
          
            - **Disabled (不可点)**
              
        - **倒计时**
          
            - 建议在按钮旁或内部显示：“预计等待 3s...” (纯心理安慰)
              
- 🟢 状态二：SUCCESS (抢购成功)
  
    - **触发条件**：后端返回 `status: "SUCCESS"` (DB 入库成功)。
      
    - **场景**：一切顺利，订单已生成。
      
    - (empty)
      
        - **按钮文案**
          
            - ✅ **“抢购成功！”**
              
        - **样式 (Style)**
          
            - **绿色 (Green)** / 高亮
              
        - **持续时间**
          
            - 展示约 2 秒的高亮反馈。
              
        - **后续变化**
          
            - **关键逻辑分支：**<br>1. **如果还有额度** (如 3/5)：2秒后变回 **“立即购买”** (允许买剩下的 2 个)。<br>2. **如果额度已满** (5/5)：变更为 **“限购已满”** (灰色不可点)。
              
        - **配额显示**
          
            - 立即将本地显示的配额从 “已抢 2/5” 更新为 **“已抢 5/5”**。
              
- 🔴 状态三：FAILED (明确失败)
  
    - **触发条件**：后端返回 `status: "FAILED"` (原因：库存不足 或 超过限购)。
      
    - **场景**：Redis 或 DB 明确拒绝了请求。
      
    - (empty)
      
        - **按钮文案**
          
            - ❌ **“库存不足”** 或 **“限购已满”**
              
        - **样式 (Style)**
          
            - **红色 (Red)** 或 **深灰色**
              
        - **交互 (Click)**
          
            - **Disabled (不可点)/2秒后也许可以买**
              
        - **恢复逻辑**
          
            - 1. 弹出 Toast 提示具体失败原因。<br>2. 将本地临时扣减的配额（Pending）加回去（回滚 UI）。<br>3. 如果是“库存不足”，按钮永久置灰。<br>4. 如果是“其他错误”，2秒后恢复为“立即购买”。
              
- 🟠 状态四：NOT_FOUND (脑裂/丢数据/超时) —— **设计的核心**
  
    - **触发条件**：轮询 `GET /status` 连续 5 秒（或 3-5 次）都返回 404/null。
      
    - **场景**：Redis 脑裂写丢失，或者网络极度拥堵。此时用户处于“不知所措”的状态。
      
    - **这里不能直接告诉用户“失败”，因为万一消息还在路上呢？**
      
    - (empty)
      
        - **按钮文案**
          
            - ⚠️ **“请求超时，点此刷新”** 或 **“查看结果”**
              
        - **样式 (Style)**
          
            - **橙色 (Orange)** / 警告色 (引起注意，但不是错误)
              
        - **图标**
          
            - 刷新图标 (↻) 或 问号 (?)
              
        - **交互 (Click)**
          
            - **Enabled (可点击)**
              
        - **点击行为**
          
            - **这是最关键的一步 (二选一策略)：**
              
    - 策略 A：保守派（推荐，绝对防超卖）
      
        - **点击动作**：不重发请求，而是**跳转**到“我的订单”页面。
          
        - **心理暗示**：“我不知道成没成，你自己去后台看看。”
          
        - **适用**：对数据一致性要求极高，宁可让用户麻烦点，也不要发重复请求。
          
    - 策略 B：激进派（自动修复脑裂，体验最好）
      
        - **点击动作**：**再次发起 POST /buy 请求**。
          
        - **核心黑科技**：**必须携带上一次的 ****`req_id` (比如 REQ_ABC)！**
          
            - 如果 Redis 真的丢数据了（脑裂）：这次点击会补上数据，用户以为是重试成功了。
              
            - 如果 Redis 没丢数据（只是慢）：这次点击会被 Lua 脚本拦截（幂等），返回“成功”，用户体验无缝衔接。
              
        - **文案变化**：点击后，按钮再次变为 **“正在排队...”** (回到状态一)。
          
- 总结：按钮状态流转图
  
    - 你可以把这段逻辑直接给前端看：
      
    - ```mermaid  
      graph TD  
          A[初始状态: 立即购买] -->|点击| B(灰色: 正在排队...)  
            
          B -->|轮询: PENDING| B  
          B -->|轮询: NOT_FOUND < 5s| B  
            
          B -->|轮询: SUCCESS| C(绿色: 抢购成功!)  
          C -->|还有额度| A  
          C -->|额度已满| D(灰色: 限购已满)  
            
          B -->|轮询: FAILED| E(红色: 库存不足)  
            
          B -->|轮询: NOT_FOUND > 5s| F(橙色: 超时/重试)  
            
          F -->|点击: 带着旧ID重试| B  
      ```
      
- 给前端开发的最终锦囊
  
    1. **“不要骗用户，但要哄用户”**：在 `NOT_FOUND` 的前几秒，哪怕后端查不到数据，也要显示“正在排队”，因为这通常是正常的物理延迟。
       
    2. **“给用户一条生路”**：超时（状态四）时，千万不要把按钮锁死在灰色。一定要变回可点击状态（橙色），让用户掌握控制权（无论是重试还是去查单）。
       
    3. **“死守 ID”**：在当前页面未刷新的情况下，**只要是针对同一笔购买意图（比如这 3 个），无论用户点多少次重试，前端传给后端的 ****`req_id` 必须永远是同一个。** 这是抵抗 Redis 脑裂的最后一道防线。
       
### 回查方案引入的接口设计，千万不要缓存穿透，不去查db

- async function pollStatus(reqId) {  
    const result = await api.get(`/seckill/status?req_id=${req_id} `)  
  }
  
- 消费成功会在redis里写一个标记为xxxfinish的key，只要这个key存在，由于我们在消费端保证了，写 Redis 的数据结构必须包含所有必要信息，让前端不需要查 DB 就能展示“抢购成功页”。
  
## 关于脑裂的解决方法

### 1. 战术层面：强制同步与“宁杀错不放过” (Strict Config)（太影响性能了，没啥必要）

- 虽然 `WAIT` 不是银弹，但通过极端的配置，我们可以将系统置于一个**“极度保守”**的状态。这符合你说的“拒绝也行”。
  
- **Q2: 如果 User_A 点了“买 5 个”，网络卡了，客户端自动重试了怎么防？**
  
    - **Client 端**：虽然发了两次请求，但因为是“重试”，所以携带的 ID **依然是 ****`REQ_999`**。
      
    - **Redis Lua**：
      
        - 第一次请求到达：`SETNX seckill:req:REQ_999` 成功，扣库存。
          
        - 第二次请求到达：`SETNX seckill:req:REQ_999` **失败**（返回 0）。
          
    - **结果**：Lua 脚本直接返回 -1，第二次请求被丢弃。库存只扣了 5 个。**数据安全**。
      
- **配置策略**：
  
    - **`min-replicas-to-write 1` (或更多)**：在 `redis.conf` 中配置。如果不满足至少 1 个 Slave 在线并同步，Master **直接拒绝写入**。这从源头阻止了“孤岛 Master”继续接收写请求。
      
    - **`WAIT 1 1000` 的错误处理**：业务服务器执行 Lua 脚本后，调用 `WAIT`。
      
        - **关键点**：如果 `WAIT` 超时（返回的已同步副本数 < 要求数），业务代码**必须**认为**交易失败**（即使 Redis Master 上可能已经扣减成功）。
          
        - **回滚/标记**：给用户报错“繁忙”，同时后台可以尝试发送一个“补偿/回滚”消息（虽然在脑裂场景下回滚可能也发不出去，但这是一个态度）。
          
- **效果**：这会导致大量的**“少卖”**（Underselling）。
  
    - 例如：Master 扣了，Slave 其实也收到了，但 ACK 慢了。系统判定失败，用户没买到。
      
    - **大厂态度**：少卖没问题，库存还在那，等流量洪峰过了或者人工补货还能卖。**超卖是绝对的技术事故。**
      
### 2. 架构层面：库存分桶 (Stock Sharding) —— 必杀技（要是有机器可以这么用）

- 这是大厂解决 Redis 集群热点和单点故障（包括脑裂风险）最常用的**物理隔离**手段。
  
- **原理**：  
  不要把 10000 个 iPhone 的库存全放在一个 Key (`stock:1001`) 上，而是拆分成 100 个 Key，分散在 Redis 集群的**不同节点（Shards）** 上。
  
    - `stock:1001:0` -> 100 个
      
    - `stock:1001:1` -> 100 个
      
    - ...
      
    - `stock:1001:99` -> 100 个
      
- **流程**：
  
    1. 用户请求进来，通过 `hash(user_id) % 100` 计算出该用户应该去哪个桶扣库存。
       
    2. 去对应的 Redis 节点执行 Lua 扣减。
       
- **如何防脑裂超卖？**
  
    - **风险稀释**：如果 Redis 集群中某一个 Master 节点发生了“脑裂”或宕机回滚，**它只影响这 1/100 的库存**。
      
    - **损失控制**：假设该节点发生故障，回滚了 5 秒的数据。在单 Key 模式下，你可能超卖 5000 单；在分桶模式下，你只超卖了 50 单。对于大厂的体量，几十单的资损是可以通过“客户关怀”（发优惠券、协商退款）低成本解决的。
      
### 3. 终极防守：数据库兜底 (DB Constraint) —— 真正的守门员（也是我们的架构使用的）

- 既然你提到了“Redis 是基准，异步下单”，那么 Redis 其实只是一个**流量漏斗**。真正的**“账本”**依然是数据库。
  
- 即使 Redis 脑裂了，放进来了 105 个人（实际库存 100），这 105 个消息最终会由 Relay 搬运到 Consumer，最后尝试插入数据库。
  
- **数据库层面的“乐观锁”或“约束”**：  
  在 Consumer 扣减数据库真实库存表时，SQL 必须这么写：
  
    - ```sql  
      UPDATE product_stock   
      SET stock = stock - 1   
      WHERE sku_id = 1001 AND stock > 0; -- 关键：stock > 0  
      ```
      
- **流程推演**：
  
    1. Redis 脑裂，多放了 5 个人进来（Redis 库存变成了负数或者回滚了，反正发出了 105 个 Outbox 消息）。
       
    2. MQ 里有 105 个订单消息。
       
    3. Consumer 处理前 100 个：成功，数据库库存归零。
       
    4. Consumer 处理第 101 个：执行上面的 SQL，`Affected Rows` 为 0（因为 `stock > 0` 不满足）。
       
    5. **处理结果**：Consumer 捕获到“库存不足”的情况，**标记该订单失败**（或者进入“排队超时”状态），并给用户发送“抱歉，抢购失败”的通知（虽然用户之前在前台看到了“排队中”甚至“抢购成功”，但最终结果以 DB 为准）。
       
- **这是大厂的底线逻辑：**  
  前端和 Redis 层的“成功”只是**“获得了入场券”**，而不是**“成交”**。最终成交以数据库扣减成功为准。只要数据库不松口，Redis 再怎么脑裂，也不会产生实际的**资金/货物超卖**，只会产生**“用户体验上的超卖”**（用户以为买到了，最后告诉他没货）。
  
### Master 虽然挂了，但在挂之前，Relay成功把消息推进了 RabbitMQ：**Redis 里的防线全部撤销了（回滚），但进攻的兵力（消息）已经到了城下（MQ）。**

- 结论是：**在一人一单和一人多单场景下，数据都不会错，绝对不会超卖或超限，但数据库（DB）会承受额外的“无效撞击”。**
  
    - 这就导致了一个现象：**Redis 失去了记忆，认为“啥也没发生”，但实际上 RabbitMQ 里已经躺着一笔有效订单了。**
      
- 场景设定：由于脑裂/重启引发的“时空分裂”
  
    1. **发生前**：用户 A 买 3 个。Redis 扣库存、加配额、入 Outbox。
       
    2. **Relay 动作**：Relay 把消息推入 RabbitMQ，**RabbitMQ 收到并持久化了**。
       
    3. **灾难发生**：Redis Master 挂了。
       
        - **关键点**：Relay 还没来得及删掉 Redis 里的 Outbox 消息（或者删了也没同步），且 Master 也没把“扣库存”和“加配额”的操作同步给 Slave。
          
    4. **新主上位**：
       
        - **Redis 状态**：库存回滚（变多），用户配额回滚（变少），Outbox 空了。
          
        - **MQ 状态**：有一条“用户 A 买 3 个”的有效消息。
          
- 情况一：一人一单（严格限购 1 个）
  
    - **风险点**：Redis 忘了用户 A 买过，如果用户 A 重试，Redis 会再次放行。
      
    - **推演流程**：
      
    1. **MQ 里的幽灵订单**：
       
        - Consumer 收到 MQ 里的消息。
          
        - **DB 执行**：`INSERT orders ...`。
          
        - **结果**：**入库成功**。用户 A 实际上已经买到了。
          
    2. **用户的第二次冲击（重试）**：
       
        - 用户前端超时，发起重试（或恶意重刷）。
          
        - **Redis（失忆版）**：查 `seckill:users`，没看到用户 A（回滚了）。**放行！**
          
        - Redis 再次发一条消息到 MQ。
          
    3. **DB 的最终防御**：
       
        - Consumer 收到 **第二条** 消息。
          
        - **DB 执行**：`INSERT orders ...`
          
        - **拦截**：触发 `UNIQUE KEY (user_id, sku_id)` 冲突。
          
        - **结果**：事务回滚，抛弃消息。
          
    - **结论**：**安全**。虽然 Redis 放进了两个请求，但 DB 利用唯一索引杀死了第二个。
      
- 情况二：一人多单 / 累计限购（例如限购 5 个）
  
    - **风险点**：Redis 忘了用户 A 刚买了 3 个（配额回滚），如果用户 A 再买 3 个，Redis 会以为 `0+3 <= 5` 放行，但实际上应该是 `3+3 > 5`。
      
    - **推演流程（假设用户原配额为 0，想买 3 个）：**
      
    1. **MQ 里的幽灵订单（第一笔，3个）**：
       
        - Consumer 收到消息。
          
        - **DB 执行**：
          
            - `INSERT orders (req_id=REQ_01)` -> 成功。
              
            - `UPDATE quota SET count = count + 3` -> 成功。
              
            - **DB 现状**：用户 A 已买 3 个。
              
    2. **用户的第二次冲击（重试，或者新下单买 3 个）**：
       
        - **场景**：用户看超时了，又下一单（REQ_02），想再买 3 个。
          
        - **Redis（失忆版）**：
          
            - 查 Quota：**是 0**（因为 Redis 回滚了）。
              
            - 判断：`0 + 3 <= 5`。**通过！**（本该拒绝的，因为实际已买 3，再买 3 就超了）。
              
            - Redis 放行，MQ 收到第二条消息（REQ_02，买 3 个）。
              
    3. **DB 的最终防御（逻辑拦截）**：
       
        - Consumer 收到 `REQ_02` (买 3 个)。
          
        - **DB 执行 Quota 更新**：
          
            - ```sql  
              -- 此时 DB 里 owned_count 已经是 3 了  
              INSERT INTO user_quota ...  
              ON DUPLICATE KEY UPDATE  
              owned_count = IF(owned_count + 3 <= 5, owned_count + 3, owned_count);  
              ```
              
        - **逻辑判断**：`3 + 3 = 6`，`6 <= 5` **不成立**。
          
        - **执行结果**：`Affected Rows = 0` (数据未变)。
          
        - **Consumer 动作**：判断更新失败，**回滚事务**。
          
    - **结论**：**安全**。虽然 Redis 的计算器算错了（基于旧数据），但 DB 的计算器是基于持久化数据的，它精准地拦截了这笔“超限”订单。
      
- 情况三：全局库存（Redis 脑裂导致“假库存”）
  
    - **风险点**：Redis 回滚了，本来剩 0 个，Redis 以为还有 10 个。
      
    - **推演流程**：
      
    1. MQ 里有 10 个订单在排队（刚好把 DB 库存扣完）。
       
    2. Redis 回滚，又放进来了 10 个用户。
       
    3. 现在 MQ 里有 20 个订单。
       
    4. **DB 消费前 10 个**：`UPDATE stock ... WHERE stock >= 1` 成功。库存归零。
       
    5. **DB 消费后 10 个**：
       
        - `UPDATE stock ... WHERE stock >= 1`。
          
        - **结果**：库存已为 0，SQL 返回 0 行受影响。
          
        - **动作**：Consumer 回滚事务，订单失败。
          
    - **结论**：**安全**。不会超卖，只是后进来的 10 个用户会被“骗”进数据库，然后被数据库无情拒绝。
      
- 总结：这种架构的本质
  
    - 你所担心的“Redis 挂了，消息发出去了”，本质上是 **Cache（缓存）与 Storage（存储）的状态不一致**。
      
    - 在这个架构中：
      
    1. **Redis 是“防波堤”**：它的任务是**尽可能**挡住不合法的请求，保护 DB。如果它坏了（脑裂/回滚），它就变成了“漏勺”，把压力漏给了 DB。
       
    2. **DB 是“守门员”**：它拥有最终解释权。
       
    - **只要你的 Consumer 端 SQL 逻辑写得够严谨（利用唯一索引、行锁、IF 条件判断），Redis 无论发生什么灾难（挂机、脑裂、数据丢光），都不会导致业务数据的错误（超卖/超限），只会导致一部分本该在 Redis 被拦截的流量，打到了 MySQL 上。**
      
    - 这也正是为什么我们在 Consumer 代码里一定要写：  
      `UPDATE ... WHERE stock > 0`  
      而不是  
      `SELECT stock; if stock > 0 { UPDATE ... }`  
      的原因——必须利用数据库的**当前读（Current Read）和原子性**来做最后的兜底。
      
